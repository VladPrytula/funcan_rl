% ============================================================================
% MASTER BIBLIOGRAPHY FOR 48-WEEK RL/MEASURE THEORY STUDY PLAN
% ============================================================================
% Citation Key Format: author:shortname:year
% Example: @folland:real_analysis:1999
%
% Tier 1: Weekly Use (Primary References)
% Tier 2: Regular Reference (Secondary Sources)
% Tier 3: Specialized Topics
% Papers: RL/ML Research Papers
% ============================================================================

% ============================================================================
% TIER 1: WEEKLY USE (PRIMARY REFERENCES)
% ============================================================================

@book{folland:real_analysis:1999,
  author    = {Folland, Gerald B.},
  title     = {Real Analysis: Modern Techniques and Their Applications},
  edition   = {2nd},
  year      = {1999},
  publisher = {Wiley-Interscience},
  series    = {Pure and Applied Mathematics},
  isbn      = {978-0471317166},
  note      = {Primary reference for measure theory, integration, Lᵖ spaces (Weeks 1-6, 10-12)}
}

@book{brezis:functional_analysis:2011,
  author    = {Brezis, Haïm},
  title     = {Functional Analysis, Sobolev Spaces and Partial Differential Equations},
  year      = {2011},
  publisher = {Springer},
  series    = {Universitext},
  isbn      = {978-0387709130},
  note      = {Primary reference for Banach/Hilbert spaces, weak convergence, Sobolev spaces (Weeks 13-24)}
}

@book{puterman:mdp:2014,
  author    = {Puterman, Martin L.},
  title     = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  edition   = {2nd},
  year      = {2014},
  publisher = {Wiley},
  series    = {Wiley Series in Probability and Statistics},
  isbn      = {978-0471727828},
  note      = {Canonical MDP reference (Weeks 25-28)}
}

@book{lattimore:bandit_algorithms:2020,
  author    = {Lattimore, Tor and Szepesvári, Csaba},
  title     = {Bandit Algorithms},
  year      = {2020},
  publisher = {Cambridge University Press},
  isbn      = {978-1108571401},
  url       = {https://tor-lattimore.com/downloads/book/book.pdf},
  note      = {Primary reference for MAB, UCB, Thompson Sampling (Weeks 29-33)}
}

% ============================================================================
% TIER 2: REGULAR REFERENCE (SECONDARY SOURCES)
% ============================================================================

@book{durrett:probability:2019,
  author    = {Durrett, Rick},
  title     = {Probability: Theory and Examples},
  edition   = {5th},
  year      = {2019},
  publisher = {Cambridge University Press},
  series    = {Cambridge Series in Statistical and Probabilistic Mathematics},
  isbn      = {978-1108473682},
  note      = {Supplementary probability theory (Weeks 1-12, 25-28)}
}

@book{levin:markov_chains:2017,
  author    = {Levin, David A. and Peres, Yuval and Wilmer, Elizabeth L.},
  title     = {Markov Chains and Mixing Times},
  edition   = {2nd},
  year      = {2017},
  publisher = {American Mathematical Society},
  isbn      = {978-1470429621},
  url       = {https://pages.uoregon.edu/dlevin/MARKOV/},
  note      = {Mixing times, coupling, MCMC (Weeks 7-12)}
}

@book{borkar:stochastic_approximation:2008,
  author    = {Borkar, Vivek S.},
  title     = {Stochastic Approximation: A Dynamical Systems Viewpoint},
  year      = {2008},
  publisher = {Springer},
  series    = {Texts and Readings in Mathematics},
  isbn      = {978-8185931630},
  note      = {ODE method, Robbins-Monro, TD learning convergence (Weeks 34-39)}
}

@book{bertsekas:rl_optimal_control:2019,
  author    = {Bertsekas, Dimitri P.},
  title     = {Reinforcement Learning and Optimal Control},
  year      = {2019},
  publisher = {Athena Scientific},
  isbn      = {978-1886529397},
  note      = {Modern RL perspective, approximate DP, policy gradients (Weeks 34-43)}
}

% ============================================================================
% TIER 3: SPECIALIZED TOPICS
% ============================================================================

@book{meyn:markov_chains:2009,
  author    = {Meyn, Sean P. and Tweedie, Richard L.},
  title     = {Markov Chains and Stochastic Stability},
  edition   = {2nd},
  year      = {2009},
  publisher = {Cambridge University Press},
  isbn      = {978-0521731829},
  note      = {General state space chains, ergodicity, stability (Week 12, 28)}
}

@book{yong:stochastic_controls:1999,
  author    = {Yong, Jiongmin and Zhou, Xun Yu},
  title     = {Stochastic Controls: Hamiltonian Systems and HJB Equations},
  year      = {1999},
  publisher = {Springer},
  series    = {Applications of Mathematics},
  isbn      = {978-0387987231},
  note      = {Continuous-time control, HJB equations (Weeks 22-24, 40)}
}

@book{bardi:viscosity_solutions:1997,
  author    = {Bardi, Martino and Capuzzo-Dolcetta, Italo},
  title     = {Optimal Control and Viscosity Solutions of Hamilton-Jacobi-Bellman Equations},
  year      = {1997},
  publisher = {Birkhäuser},
  series    = {Systems \& Control: Foundations \& Applications},
  isbn      = {978-0817637507},
  note      = {Viscosity solutions for HJB (Week 24)}
}

@book{sutton:reinforcement_learning:2018,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  title     = {Reinforcement Learning: An Introduction},
  edition   = {2nd},
  year      = {2018},
  publisher = {MIT Press},
  isbn      = {978-0262039246},
  url       = {http://incompleteideas.net/book/the-book-2nd.html},
  note      = {Narrative guide, intuition building (all phases)}
}

@book{yosida:functional_analysis:1995,
  author    = {Yosida, Kôsaku},
  title     = {Functional Analysis},
  edition   = {6th},
  year      = {1995},
  publisher = {Springer},
  series    = {Classics in Mathematics},
  isbn      = {978-3540586548},
  note      = {Advanced functional analysis, semigroup theory (Weeks 16-18)}
}

@book{adams:sobolev_spaces:2003,
  author    = {Adams, Robert A. and Fournier, John J. F.},
  title     = {Sobolev Spaces},
  edition   = {2nd},
  year      = {2003},
  publisher = {Academic Press},
  series    = {Pure and Applied Mathematics},
  isbn      = {978-0120441433},
  note      = {Comprehensive Sobolev space reference (Weeks 19-21)}
}

% ============================================================================
% REINFORCEMENT LEARNING PAPERS (KEY ALGORITHMS)
% ============================================================================

@article{mnih:dqn:2015,
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and others},
  title   = {Human-level control through deep reinforcement learning},
  journal = {Nature},
  year    = {2015},
  volume  = {518},
  number  = {7540},
  pages   = {529--533},
  doi     = {10.1038/nature14236},
  note    = {Deep Q-Network (DQN), experience replay, target networks}
}

@inproceedings{schulman:ppo:2017,
  author    = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title     = {Proximal Policy Optimization Algorithms},
  booktitle = {arXiv preprint arXiv:1707.06347},
  year      = {2017},
  url       = {https://arxiv.org/abs/1707.06347},
  note      = {PPO, current standard for policy gradient methods}
}

@inproceedings{haarnoja:sac:2018,
  author    = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018},
  pages     = {1861--1870},
  note      = {Soft Actor-Critic (SAC), maximum entropy RL}
}

@article{silver:alphago:2016,
  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  title   = {Mastering the game of Go with deep neural networks and tree search},
  journal = {Nature},
  year    = {2016},
  volume  = {529},
  number  = {7587},
  pages   = {484--489},
  doi     = {10.1038/nature16961},
  note    = {AlphaGo, MCTS + deep learning}
}

@article{silver:alphazero:2018,
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  title   = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal = {Science},
  year    = {2018},
  volume  = {362},
  number  = {6419},
  pages   = {1140--1144},
  doi     = {10.1126/science.aar6404},
  note    = {AlphaZero generalization (Capstone project inspiration, Weeks 44-48)}
}

@inproceedings{schrittwieser:muzero:2020,
  author    = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  booktitle = {Nature},
  year      = {2020},
  volume    = {588},
  pages     = {604--609},
  doi       = {10.1038/s41586-020-03051-4},
  note      = {MuZero, model-based RL with learned dynamics}
}

@article{hafner:dreamer:2020,
  author  = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  title   = {Dream to Control: Learning Behaviors by Latent Imagination},
  journal = {International Conference on Learning Representations (ICLR)},
  year    = {2020},
  url     = {https://arxiv.org/abs/1912.01603},
  note    = {Dreamer, world models for RL}
}

% ============================================================================
% CLASSIC RL BENCHMARKS & FOUNDATIONAL PAPERS
% ============================================================================

@article{barto:cart_pole:1983,
  author  = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  title   = {Neuronlike adaptive elements that can solve difficult learning control problems},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  year    = {1983},
  volume  = {SMC-13},
  number  = {5},
  pages   = {834--846},
  doi     = {10.1109/TSMC.1983.6313077},
  note    = {Original CartPole problem formulation}
}

% ============================================================================
% FUNCTIONAL ANALYSIS & MEASURE THEORY (CLASSICAL REFERENCES)
% ============================================================================

@book{rudin:real_complex:1987,
  author    = {Rudin, Walter},
  title     = {Real and Complex Analysis},
  edition   = {3rd},
  year      = {1987},
  publisher = {McGraw-Hill},
  isbn      = {978-0070542341},
  note      = {Standard real analysis reference (assumed background)}
}

@book{rudin:functional_analysis:1991,
  author    = {Rudin, Walter},
  title     = {Functional Analysis},
  edition   = {2nd},
  year      = {1991},
  publisher = {McGraw-Hill},
  series    = {International Series in Pure and Applied Mathematics},
  isbn      = {978-0070542365},
  note      = {Classical functional analysis (Weeks 13-18)}
}

@book{royden:real_analysis:2010,
  author    = {Royden, H. L. and Fitzpatrick, Patrick},
  title     = {Real Analysis},
  edition   = {4th},
  year      = {2010},
  publisher = {Prentice Hall},
  isbn      = {978-0131437470},
  note      = {Alternative real analysis reference}
}

% ============================================================================
% PROBABILITY THEORY (ADDITIONAL REFERENCES)
% ============================================================================

@book{billingsley:probability:2012,
  author    = {Billingsley, Patrick},
  title     = {Probability and Measure},
  edition   = {Anniversary},
  year      = {2012},
  publisher = {Wiley},
  series    = {Wiley Series in Probability and Statistics},
  isbn      = {978-1118122372},
  note      = {Classic probability theory text}
}

@book{kallenberg:foundations:2021,
  author    = {Kallenberg, Olav},
  title     = {Foundations of Modern Probability},
  edition   = {3rd},
  year      = {2021},
  publisher = {Springer},
  series    = {Probability Theory and Stochastic Modelling},
  isbn      = {978-3030618704},
  note      = {Encyclopedic probability reference}
}

% ============================================================================
% STOCHASTIC APPROXIMATION & CONVERGENCE
% ============================================================================

@book{kushner:stochastic_approximation:2003,
  author    = {Kushner, Harold J. and Yin, G. George},
  title     = {Stochastic Approximation and Recursive Algorithms and Applications},
  edition   = {2nd},
  year      = {2003},
  publisher = {Springer},
  series    = {Applications of Mathematics},
  isbn      = {978-0387008943},
  note      = {Comprehensive SA reference (Weeks 34-39)}
}

@article{robbins:stochastic_approximation:1951,
  author  = {Robbins, Herbert and Monro, Sutton},
  title   = {A Stochastic Approximation Method},
  journal = {The Annals of Mathematical Statistics},
  year    = {1951},
  volume  = {22},
  number  = {3},
  pages   = {400--407},
  doi     = {10.1214/aoms/1177729586},
  note    = {Original Robbins-Monro algorithm}
}

% ============================================================================
% OPTIMAL CONTROL & DYNAMIC PROGRAMMING
% ============================================================================

@book{bertsekas:dynamic_programming:2017,
  author    = {Bertsekas, Dimitri P.},
  title     = {Dynamic Programming and Optimal Control},
  edition   = {4th},
  year      = {2017},
  publisher = {Athena Scientific},
  volume    = {1 \& 2},
  isbn      = {978-1886529434},
  note      = {Canonical DP reference}
}

@book{fleming:controlled_diffusions:2006,
  author    = {Fleming, Wendell H. and Soner, Halil Mete},
  title     = {Controlled Markov Processes and Viscosity Solutions},
  edition   = {2nd},
  year      = {2006},
  publisher = {Springer},
  series    = {Stochastic Modelling and Applied Probability},
  isbn      = {978-0387260457},
  note      = {Continuous-time control, viscosity solutions}
}

% ============================================================================
% MEAN-FIELD GAMES & ADVANCED TOPICS
% ============================================================================

@book{carmona:mean_field_games:2018,
  author    = {Carmona, René and Delarue, François},
  title     = {Probabilistic Theory of Mean Field Games with Applications I-II},
  year      = {2018},
  publisher = {Springer},
  series    = {Probability Theory and Stochastic Modelling},
  isbn      = {978-3319564371},
  note      = {Mean-field games (Week 41)}
}

% ============================================================================
% DEEP LEARNING & NEURAL NETWORKS
% ============================================================================

@book{goodfellow:deep_learning:2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title     = {Deep Learning},
  year      = {2016},
  publisher = {MIT Press},
  isbn      = {978-0262035613},
  url       = {https://www.deeplearningbook.org/},
  note      = {Standard deep learning reference (Weeks 40-48)}
}

% ============================================================================
% POLICY GRADIENT METHODS
% ============================================================================

@inproceedings{sutton:policy_gradient:2000,
  author    = {Sutton, Richard S. and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2000},
  volume    = {13},
  pages     = {1057--1063},
  note      = {Policy gradient theorem, REINFORCE with baseline}
}

@article{williams:reinforce:1992,
  author  = {Williams, Ronald J.},
  title   = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  journal = {Machine Learning},
  year    = {1992},
  volume  = {8},
  pages   = {229--256},
  doi     = {10.1007/BF00992696},
  note    = {REINFORCE algorithm (Week 37-38)}
}

@inproceedings{silver:dpg:2014,
  author    = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  title     = {Deterministic Policy Gradient Algorithms},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2014},
  pages     = {387--395},
  note      = {Deterministic policy gradients}
}

% ============================================================================
% TEMPORAL DIFFERENCE LEARNING
% ============================================================================

@phdthesis{sutton:td_learning:1984,
  author = {Sutton, Richard S.},
  title  = {Temporal Credit Assignment in Reinforcement Learning},
  school = {University of Massachusetts Amherst},
  year   = {1984},
  note   = {Original TD learning thesis (Week 35-36)}
}

@article{tsitsiklis:td_convergence:1997,
  author  = {Tsitsiklis, John N. and Van Roy, Benjamin},
  title   = {An Analysis of Temporal-Difference Learning with Function Approximation},
  journal = {IEEE Transactions on Automatic Control},
  year    = {1997},
  volume  = {42},
  number  = {5},
  pages   = {674--690},
  doi     = {10.1109/9.580874},
  note    = {TD convergence with function approximation (Week 36)}
}

% ============================================================================
% END OF BIBLIOGRAPHY
% ============================================================================
