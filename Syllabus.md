# A 48-Week Rigorous Syllabus: From Measure Theory to Reinforcement Learning

## Structural Principles

**Daily Time Allocation**: 90 minutes, Monday-Friday (weekends off)

**Adaptive Weekly Template**:

- **Monday-Wednesday**: 40 min reading + 40 min proof/exercise + 10 min reflection note
- **Thursday**: 30 min reading + 60 min extended proof or multi-part exercise
- **Friday**: 20 min reading + 30 min proof review + 40 min coding synthesis

**Phase Structure**: Six major phases with clear mathematical and RL objectives

---

## Phase I: Measure-Theoretic Foundations (Weeks 1-6)

**Objective**: Rapid but rigorous traversal of measure theory, emphasizing aspects critical for probability and RL. Given your background, this serves as structured review with targeted depth.

### Week 1: Ïƒ-Algebras, Measures, and Measurable Functions

**Primary Reference**: Folland Â§1.1-1.4

**Monday-Wednesday**:

- Ïƒ-algebras and their generation (monotone classes, Dynkin's Ï€-Î» theorem)
- Measures, outer measures, and CarathÃ©odory extension theorem
- Measurable functions and their arithmetic/composition properties

**Thursday**: Extended proof of CarathÃ©odory extension theorem (construction of Lebesgue measure from outer measure on rectangles)

**Friday**:

- Review: Why Ïƒ-algebras encode observability in RL
- Code: Implement discrete Ïƒ-algebra generator, visualize set operations

**Anchor Exercises**:

1. Prove the Ï€-Î» theorem: if ğ’« is a Ï€-system and â„’ is a Î»-system with ğ’« âŠ† â„’, then Ïƒ(ğ’«) âŠ† â„’
2. Show that the Borel Ïƒ-algebra on â„â¿ is generated by open balls (or by half-open rectangles)

### Week 2: Integration and Convergence Theorems

**Primary Reference**: Folland Â§2.1-2.4

**Monday-Wednesday**:

- Construction of the Lebesgue integral (simple functions â†’ non-negative â†’ general)
- Monotone Convergence Theorem (MCT)
- Fatou's Lemma and Dominated Convergence Theorem (DCT)

**Thursday**: Complete proof of DCT from first principles, emphasizing the role of dominating function

**Friday**:

- Review: How DCT enables interchange of expectation and limits in RL (e.g., policy evaluation)
- Code: Numerical experiments showing failure of pointwise limit interchange without domination

**Anchor Exercises**:

1. Prove that if fâ‚™ â†’ f in LÂ¹, there exists a subsequence converging almost everywhere
2. Construct explicit counterexamples showing necessity of each hypothesis in MCT and DCT

### Week 3: Product Measures and Fubini's Theorem

**Primary Reference**: Folland Â§2.4-2.5, Durrett Â§1.7

**Monday-Wednesday**:

- Product Ïƒ-algebras and product measures
- Fubini-Tonelli theorem (complete statement and proof strategy)
- Applications to transition kernels in MDPs

**Thursday**: Full proof of Fubini's theorem for Ïƒ-finite measures

**Friday**:

- Review: Fubini justifies swapping âˆ‘â‚›â€² âˆ«â‚ in Bellman operator
- Code: Monte Carlo integration in 2D demonstrating Fubini, compare with iterated integration

**Anchor Exercises**:

1. Prove the Tonelli form: if f â‰¥ 0 is measurable on X Ã— Y, then âˆ«(âˆ«f dÎ¼)dÎ½ = âˆ«(âˆ«f dÎ½)dÎ¼
2. Show Ïƒ-finiteness is necessary: construct counterexample with counting measure on uncountable set

### Week 4: Radon-Nikodym and Signed Measures

**Primary Reference**: Folland Â§3.1-3.4

**Monday-Wednesday**:

- Signed measures and Hahn decomposition
- Absolute continuity and singularity of measures
- Radon-Nikodym theorem: existence of density dÎ¼/dÎ½

**Thursday**: Complete proof of Radon-Nikodym theorem via maximal element in Hilbert space

**Friday**:

- Review: Radon-Nikodym underlies importance sampling and likelihood ratios in RL
- Code: Implement Radon-Nikodym density estimation numerically for simple examples

**Anchor Exercises**:

1. Prove the Hahn decomposition theorem for signed measures
2. Show that Radon-Nikodym derivative satisfies chain rule: if Î¼ â‰ª Î½ â‰ª Î», then d(Î¼/Î») = (dÎ¼/dÎ½)(dÎ½/Î») a.e.

### Week 5: Láµ– Spaces and Duality

**Primary Reference**: Folland Â§6.1-6.3

**Monday-Wednesday**:

- Definition of Láµ–(Î¼) spaces, HÃ¶lder and Minkowski inequalities
- Completeness of Láµ– (Riesz-Fischer theorem)
- Duality: (Láµ–)* â‰… Lq for p âˆˆ [1,âˆ), 1/p + 1/q = 1

**Thursday**: Prove Riesz representation theorem for (Láµ–)* when 1 < p < âˆ

**Friday**:

- Review: Value functions live in Lâˆ, policy gradients involve LÂ² inner products
- Code: Visualize convergence in different Láµ– norms, demonstrate non-convergence in one norm vs another

**Anchor Exercises**:

1. Prove HÃ¶lder's inequality via convexity (Young's inequality for products)
2. Show that Lâˆ is not separable, while Láµ– for p < âˆ is separable (on Ïƒ-finite measure spaces)

### Week 6: Probability Spaces and Conditional Expectation

**Primary Reference**: Durrett Â§1.1-1.6, Â§5.1

**Monday-Wednesday**:

- Probability spaces, random variables, and distributions
- Independence and product measures
- Conditional expectation as orthogonal projection in LÂ²
- Properties: tower property, taking out what is known

**Thursday**: Prove existence and uniqueness of conditional expectation using Hilbert space projection

**Friday**:

- Review: E[R|s,a] in MDPs is conditional expectation; tower property gives Bellman consistency
- Code: Simulate conditional expectation numerically, verify tower property empirically

**Anchor Exercises**:

1. Prove that E[X|ğ’¢] minimizes E[(X - Y)Â²] over all ğ’¢-measurable Y
2. Establish the tower property: E[E[X|ğ’¢]|â„‹] = E[X|â„‹] when â„‹ âŠ† ğ’¢

---

## Phase II: Markov Chains and Ergodic Theory (Weeks 7-12)

**Objective**: Master discrete-time Markov chains on finite and countable spaces, establishing the probabilistic backbone of MDPs.

### Week 7: Finite Markov Chains - Fundamentals

**Primary Reference**: Levin-Peres-Wilmer (LPW) Ch 1-4

**Monday-Wednesday**:

- Transition matrices and Chapman-Kolmogorov equation
- Irreducibility, aperiodicity, and communication classes
- Stationary distributions and detailed balance

**Thursday**: Prove existence and uniqueness of stationary distribution for irreducible finite chains

**Friday**:

- Review: Stationary distribution is "average" state visitation under policy
- Code: Implement random walk on graphs, compute stationary distribution via eigenvalue and via simulation

**Anchor Exercises**:

1. Prove that irreducible finite chain has unique stationary distribution Ï€ with Ï€áµ¢ > 0 for all i
2. Show detailed balance (Ï€áµ¢Páµ¢â±¼ = Ï€â±¼Pâ±¼áµ¢) implies Ï€ is stationary

### Week 8: Convergence and Mixing Times

**Primary Reference**: LPW Ch 5-7, 12

**Monday-Wednesday**:

- Total variation distance and coupling
- Convergence to stationarity: coupling lemma
- Mixing time definition and examples

**Thursday**: Prove coupling inequality: d(t) â‰¤ P(Xâ‚(t) â‰  Xâ‚‚(t)) for chains started from coupling (Xâ‚,Xâ‚‚)

**Friday**:

- Review: Mixing time bounds sample complexity in RL exploration
- Code: Implement coupling for simple chains, visualize convergence to stationarity in TV norm

**Anchor Exercises**:

1. Compute mixing time for random walk on cycle graph Câ‚™
2. Prove that for reversible chains, Ï„_mix â‰¤ (Î»â‚‚*)â»Â¹ log(1/(Ï€â‚˜áµ¢â‚™Î´)) where Î»â‚‚* = 1 - Î»â‚‚ is spectral gap

### Week 9: Markov Chain Monte Carlo

**Primary Reference**: LPW Ch 8-9, 13-14

**Monday-Wednesday**:

- Metropolis-Hastings algorithm and its variants
- Gibbs sampling
- Path coupling method for bounding mixing

**Thursday**: Prove correctness of Metropolis-Hastings: constructed chain has desired stationary distribution

**Friday**:

- Review: MCMC samples from complex distributions (e.g., Boltzmann exploration in RL)
- Code: Implement Metropolis-Hastings for 2D Ising model, verify convergence to Gibbs measure

**Anchor Exercises**:

1. Design Metropolis-Hastings algorithm to sample from Ï€(x) âˆ exp(-U(x)) on finite state space
2. Prove detailed balance for Gibbs sampler on product space

### Week 10: Countable State Spaces - Transience and Recurrence

**Primary Reference**: Durrett Â§6.1-6.4

**Monday-Wednesday**:

- Classification: transient vs recurrent states
- Null recurrence vs positive recurrence
- First passage times and hitting probabilities

**Thursday**: Prove PÃ³lya's theorem: random walk on â„¤áµˆ is recurrent iff d â‰¤ 2

**Friday**:

- Review: Recurrence structure of state space affects exploration in RL
- Code: Simulate random walks in dimensions 1,2,3; empirically verify recurrence/transience

**Anchor Exercises**:

1. Show that P(return to 0|start at 0) = âˆ‘â‚™ Pâ‚€â‚€â½â¿â¾ / (1 + âˆ‘â‚™ Pâ‚€â‚€â½â¿â¾)
2. Prove that in finite state space, not all states can be transient

### Week 11: Ergodic Theorems

**Primary Reference**: Durrett Â§6.5-6.7, Meyn-Tweedie Ch 17

**Monday-Wednesday**:

- Strong law for Markov chains
- Birkhoff's ergodic theorem (statement for measure-preserving transformations)
- Application to time averages in Markov chains

**Thursday**: Prove Birkhoff ergodic theorem for finite state space via subadditive ergodic theorem

**Friday**:

- Review: Ergodic theorem justifies Monte Carlo policy evaluation (time average = ensemble average)
- Code: Verify ergodic theorem numerically for various chains; demonstrate failure for non-ergodic chain

**Anchor Exercises**:

1. For irreducible positive recurrent chain, prove (1/n)âˆ‘áµ¢â‚Œâ‚â¿ f(Xáµ¢) â†’ âˆ‘â‚“ Ï€â‚“f(x) almost surely
2. Show necessity of positive recurrence: construct null recurrent chain where time average exists but differs from spatial average

### Week 12: General State Spaces - Introduction

**Primary Reference**: Meyn-Tweedie Ch 3-4 (selected topics)

**Monday-Wednesday**:

- Transition kernels on general measurable spaces
- Feller property and weak convergence
- Irreducibility and aperiodicity in general spaces

**Thursday**: Prove that Feller property implies weak continuity in initial distribution

**Friday**:

- Review: Continuous state RL requires general state space theory
- Code: Implement Markov chain on [0,1] with continuous kernel, visualize evolution of density

**Anchor Exercises**:

1. Define Ï†-irreducibility and show every finite chain is Ï†-irreducible for counting measure Ï†
2. Verify Feller property for Gaussian random walk: Xâ‚™â‚Šâ‚ = Xâ‚™ + Îµâ‚™ with Îµâ‚™ ~ N(0,ÏƒÂ²)

---

## Phase III: Functional Analysis and Operator Theory (Weeks 13-18)

**Objective**: Establish the function space framework where Bellman operators and value functions reside. Deep dive into Banach and Hilbert spaces.

### Week 13: Normed Spaces and Banach Spaces

**Primary Reference**: Brezis Ch 1, Â§1.1-1.3

**Monday-Wednesday**:

- Definition and examples of normed vector spaces
- Completeness and Banach spaces
- Finite-dimensional spaces: equivalence of norms

**Thursday**: Prove all norms on â„â¿ are equivalent (any two induce same topology)

**Friday**:

- Review: Space of bounded functions â„¬(ğ’®) with supremum norm is Banach; value iteration works here
- Code: Visualize unit balls in â„Â² for various norms (LÂ¹, LÂ², Lâˆ)

**Anchor Exercises**:

1. Show C([0,1]) with supremum norm is complete but C([0,1]) with LÂ¹ norm is not
2. Prove that every finite-dimensional normed space is complete

### Week 14: Linear Operators and Dual Spaces

**Primary Reference**: Brezis Ch 1, Â§1.4-1.6

**Monday-Wednesday**:

- Bounded linear operators, operator norm
- Dual space and Hahn-Banach theorem (extension form)
- Weak and weak* topologies

**Thursday**: Prove Hahn-Banach theorem (geometric form: separating hyperplane)

**Friday**:

- Review: Linear value functions correspond to functionals; duality essential for policy gradients
- Code: Implement linear functionals on finite-dimensional spaces, visualize separating hyperplanes

**Anchor Exercises**:

1. If X is Banach and T:X â†’ Y linear with closed graph, prove T is bounded (closed graph theorem)
2. Compute dual of câ‚€ (sequences converging to 0) and show (câ‚€)* â‰… â„“Â¹

### Week 15: Three Fundamental Theorems

**Primary Reference**: Brezis Ch 2, Â§2.1-2.7

**Monday-Wednesday**:

- Uniform Boundedness Principle (Banach-Steinhaus)
- Open Mapping Theorem and Inverse Mapping Theorem
- Closed Graph Theorem

**Thursday**: Prove Uniform Boundedness Principle via Baire category theorem

**Friday**:

- Review: UBP implies pointwise bounded sequence of Bellman backups is uniformly bounded
- Code: Construct example of pointwise bounded but not uniformly bounded sequence on infinite-dimensional space

**Anchor Exercises**:

1. Use UBP to show: if {Tâ‚™} âŠ‚ â„’(X,Y) and Tâ‚™x converges for each x, then T := lim Tâ‚™ is bounded
2. Prove that surjective bounded operator between Banach spaces is open

### Week 16: Hilbert Spaces

**Primary Reference**: Brezis Ch 5, Â§5.1-5.5

**Monday-Wednesday**:

- Inner product spaces and Cauchy-Schwarz inequality
- Orthogonality and projection theorem
- Riesz representation: H* â‰… H

**Thursday**: Prove projection theorem: every closed convex subset of Hilbert space contains unique nearest point to any given point

**Friday**:

- Review: Least-squares TD uses orthogonal projection in feature space (Hilbert space)
- Code: Implement orthogonal projection onto subspaces in â„â¿, visualize geometry

**Anchor Exercises**:

1. Prove Parallelogram law: â€–x+yâ€–Â² + â€–x-yâ€–Â² = 2(â€–xâ€–Â² + â€–yâ€–Â²) characterizes inner product spaces
2. Show that weak convergence xâ‚™ â‡€ x and â€–xâ‚™â€– â†’ â€–xâ€– together imply strong convergence xâ‚™ â†’ x

### Week 17: Compact Operators and Spectral Theory

**Primary Reference**: Brezis Ch 6, Â§6.1-6.5

**Monday-Wednesday**:

- Compact operators and Fredholm alternative
- Spectrum of bounded operator
- Spectral theorem for compact self-adjoint operators

**Thursday**: Prove spectral theorem: compact self-adjoint T on Hilbert space H has orthonormal eigenbasis

**Friday**:

- Review: Compactness of certain Bellman operators enables finite-rank approximation
- Code: Compute spectrum of discrete Laplacian, verify eigenfunction expansion

**Anchor Exercises**:

1. Show composition of bounded operator with compact operator is compact
2. For compact self-adjoint T, prove Tx = Î»x with Î» â‰  0 iff Î» is eigenvalue

### Week 18: Contraction Mappings and Fixed Points

**Primary Reference**: Brezis Ch 9, Â§9.1-9.3; Puterman Appendix A

**Monday-Wednesday**:

- Metric spaces and completeness
- Banach Fixed Point Theorem (Contraction Mapping Theorem)
- Applications to differential and integral equations

**Thursday**: Prove Banach Fixed Point Theorem with explicit rate of convergence

**Friday**:

- Review: Bellman operator T^Ï€ is Î³-contraction in Lâˆ, yielding unique fixed point V^Ï€
- Code: Visualize fixed point iteration for various contractions, demonstrate linear convergence

**Anchor Exercises**:

1. Prove that if T:X â†’ X is contraction with fixed point x*, then d(Tâ¿xâ‚€, x*) â‰¤ Lâ¿d(xâ‚€,x*)/(1-L)
2. Show non-expansive map on compact metric space has fixed point (Brouwer via approximation)

---

## Phase IV: Sobolev Spaces, PDEs, and Control Theory (Weeks 19-24)

**Objective**: Deep exploration of Sobolev spaces and variational methods, connecting to Hamilton-Jacobi-Bellman equations and continuous control. This is where your PDE background shines.

### Week 19: Weak Derivatives and Sobolev Spaces W^{k,p}

**Primary Reference**: Brezis Ch 8, Â§8.1-8.3

**Monday-Wednesday**:

- Weak derivatives: definition via integration by parts
- Sobolev spaces W^{k,p}(Î©) and their basic properties
- Relationship between classical and weak solutions

**Thursday**: Prove that W^{1,p}(Î©) with â€–uâ€– = (â€–uâ€–â‚šáµ– + â€–âˆ‡uâ€–â‚šáµ–)^{1/p} is Banach space

**Friday**:

- Review: Neural networks produce W^{1,âˆ} functions (Lipschitz); Sobolev regularity affects approximation quality
- Code: Compute weak derivatives numerically via finite differences, compare with classical derivatives

**Anchor Exercises**:

1. Show that if u âˆˆ W^{1,p}(Î©) and Î© is bounded Lipschitz, then Î³u (trace) is well-defined in L^p(âˆ‚Î©)
2. Prove density of C^âˆâ‚–(Î©) in W^{1,p}(Î©) when Î© has smooth boundary

### Week 20: Sobolev Embeddings and Compactness

**Primary Reference**: Brezis Ch 9, Â§9.1-9.4

**Monday-Wednesday**:

- Sobolev embedding theorem: W^{1,p}(â„â¿) â†ª L^q(â„â¿) for appropriate q
- Rellich-Kondrachov compact embedding theorem
- PoincarÃ© and PoincarÃ©-Wirtinger inequalities

**Thursday**: Prove Gagliardo-Nirenberg-Sobolev inequality: if p < n, then W^{1,p}(â„â¿) â†ª L^{p*}(â„â¿) where 1/p* = 1/p - 1/n

**Friday**:

- Review: Sobolev embeddings ensure policies in W^{1,p} have bounded supremum norm (needed for stability)
- Code: Numerically verify Sobolev embeddings for functions on unit interval

**Anchor Exercises**:

1. Prove PoincarÃ© inequality on bounded connected Î©: â€–u - Å«â€–_{LÂ²} â‰¤ Câ€–âˆ‡uâ€–_{LÂ²} where Å« = mean of u
2. Show Rellich theorem implies: bounded sequence in W^{1,p}(Î©) has L^p-convergent subsequence (when p < n*)

### Week 21: Variational Formulations and Weak Solutions

**Primary Reference**: Brezis Ch 8, Â§8.4-8.6; Ch 9, Â§9.5

**Monday-Wednesday**:

- Lax-Milgram theorem and weak solutions to elliptic PDEs
- Energy minimization and Euler-Lagrange equations
- Regularity theory: weak solutions are stronger

**Thursday**: Prove Lax-Milgram theorem: coercive continuous bilinear form on Hilbert space gives unique weak solution to Lu = f

**Friday**:

- Review: Value function in continuous control solves variational problem; weak formulation enables numerical methods
- Code: Solve Poisson equation -Î”u = f using finite elements (weak formulation)

**Anchor Exercises**:

1. For -Î”u + u = f in Î© with u = 0 on âˆ‚Î©, write weak formulation and prove existence/uniqueness via Lax-Milgram
2. Show that weak solution in W^{1,2}_0(Î©) of -Î”u = f âˆˆ LÂ²(Î©) actually belongs to W^{2,2}(Î©) (HÂ² regularity)

### Week 22: Hamilton-Jacobi Equations and Viscosity Solutions

**Primary Reference**: Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions" Ch 1-2 (or similar reference)

**Monday-Wednesday**:

- First-order Hamilton-Jacobi equations
- Characteristics and method of characteristics (when it works)
- Need for weak solutions: shocks and non-uniqueness

**Thursday**: Derive Hamilton-Jacobi-Bellman equation from dynamic programming principle in continuous time

**Friday**:

- Review: HJB equation is PDE satisfied by value function; viscosity solutions handle non-smoothness
- Code: Solve HJ equation âˆ‚â‚œu + H(âˆ‡u) = 0 numerically via upwind scheme, observe shock formation

**Anchor Exercises**:

1. For u_t + H(u_x) = 0 with H convex, derive Rankine-Hugoniot condition for shock propagation
2. Show that smooth solution to HJB equation gives optimal control via feedback law a*(s) = arg max H(s, âˆ‡V(s), a)

### Week 23: Viscosity Solutions - Theory

**Primary Reference**: Bardi-Capuzzo Dolcetta Ch 3-4; or Crandall-Ishii-Lions (User's Guide)

**Monday-Wednesday**:

- Definition of viscosity sub/super solutions
- Comparison principle for viscosity solutions
- Existence and uniqueness theorems

**Thursday**: Prove comparison principle for viscosity solutions of Hamilton-Jacobi equation (simplified case)

**Friday**:

- Review: Viscosity solution is "right" notion for value function; DQN approximates viscosity solution
- Code: Implement Godunov scheme for Hamilton-Jacobi, verify convergence to viscosity solution

**Anchor Exercises**:

1. Verify that u(x,t) = min_{y} {|x-y|Â²/(2t) + uâ‚€(y)} is viscosity solution to heat equation for smooth uâ‚€
2. Prove uniqueness of viscosity solution to u + H(âˆ‡u) = 0 when H is coercive

### Week 24: Optimal Control and HJB in Continuous Time

**Primary Reference**: Yong-Zhou "Stochastic Controls" Ch 3-4, or Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:

- Controlled SDEs and diffusions
- Dynamic programming in continuous time
- HJB equation for stochastic control: ÏV - ğ’œV = max_a {r(s,a)} where ğ’œ is infinitesimal generator

**Thursday**: Derive HJB equation for stochastic control problem with Brownian noise via ItÃ´'s formula

**Friday**:

- Review: Actor-Critic algorithms solve HJB numerically; connection to policy gradient
- Code: Solve simple LQR problem analytically via HJB, compare with Riccati equation solution

**Anchor Exercises**:

1. For linear-quadratic regulator dx = (Ax + Ba)dt + ÏƒdW with cost âˆ«(x'Qx + a'Ra)dt, derive Riccati equation from HJB
2. Verify verification theorem: if V solves HJB with feedback control a*(s), then V is value function

---

## Phase V: Markov Decision Processes and Dynamic Programming (Weeks 25-28)

**Objective**: Rigorous MDP theory where all prior mathematics pays dividends. Existence, uniqueness, convergence.

### Week 25: MDP Formalism and Bellman Equations

**Primary Reference**: Puterman Ch 3-4, Bertsekas Vol I Ch 1-2

**Monday-Wednesday**:

- Formal definition: (ğ’®, ğ’œ, P, R, Î³) with measurability requirements
- Policies (deterministic, stationary, history-dependent)
- Value functions V^Ï€ and Q^Ï€; Bellman consistency equations
- Bellman optimality equation

**Thursday**: Prove that V^Ï€ is unique fixed point of Bellman operator T^Ï€V = R^Ï€ + Î³P^Ï€V

**Friday**:

- Review: Everything from measure theory to functional analysis culminates here
- Code: Implement gridworld MDP, verify Bellman equations numerically

**Anchor Exercises**:

1. Prove T^Ï€ is Î³-contraction in (â„¬(ğ’®), â€–Â·â€–âˆ): â€–T^Ï€V - T^Ï€Wâ€–âˆ â‰¤ Î³â€–V - Wâ€–âˆ
2. Show that Q^Ï€ satisfies Q^Ï€(s,a) = R(s,a) + Î³âˆ‘_{s'} P(s'|s,a) âˆ‘_a' Ï€(a'|s')Q^Ï€(s',a')

### Week 26: Optimal Policies and Value Iteration

**Primary Reference**: Puterman Ch 6, Bertsekas Vol I Ch 1

**Monday-Wednesday**:

- Existence of optimal policies for discounted infinite-horizon MDPs
- Value iteration algorithm and convergence analysis
- Optimality equations and policy improvement

**Thursday**: Prove convergence of value iteration: Vâ‚™ â†’ V* at rate â€–Vâ‚™ - V*â€– â‰¤ Î³â¿â€–Vâ‚€ - V*â€–/(1-Î³)

**Friday**:

- Review: Contraction mapping theorem is why value iteration works
- Code: Implement value iteration on various MDPs, plot convergence in sup norm

**Anchor Exercises**:

1. Prove policy improvement theorem: if Q^Ï€(s, Ï€'(s)) â‰¥ V^Ï€(s) for all s, then V^{Ï€'} â‰¥ V^Ï€
2. Show that greedy policy with respect to V* is optimal: Ï€*(s) âˆˆ arg max_a Q*(s,a)

### Week 27: Policy Iteration and Linear Programming

**Primary Reference**: Puterman Ch 6-7

**Monday-Wednesday**:

- Policy iteration algorithm: evaluation + improvement
- Convergence in finite steps for finite MDPs
- Linear programming formulation of MDP

**Thursday**: Prove policy iteration converges in finite steps when |ğ’®| Ã— |ğ’œ| < âˆ

**Friday**:

- Review: Policy iteration is Newton's method on Bellman optimality
- Code: Implement policy iteration, compare with value iteration on computational efficiency

**Anchor Exercises**:

1. Prove that policy evaluation via solving (I - Î³P^Ï€)V = R^Ï€ gives V^Ï€
2. Formulate MDP as LP: maximize âˆ‘_s Î½(s)V(s) subject to V(s) â‰¥ R(s,a) + Î³âˆ‘_s' P(s'|s,a)V(s') for all s,a

### Week 28: Average Reward MDPs and Ergodic Theory

**Primary Reference**: Puterman Ch 8, Meyn-Tweedie Ch 17

**Monday-Wednesday**:

- Average reward criterion: lim (1/n)E[âˆ‘áµ¢â‚Œâ‚â¿ R(sáµ¢,aáµ¢)]
- Existence of bias and gain
- Connection to ergodic theorems for Markov chains

**Thursday**: Prove that for unichain MDP, average reward equals (stationary distribution)Â·(reward vector)

**Friday**:

- Review: Average reward relevant for continuing tasks (no discounting)
- Code: Compute average reward via simulation, compare with solving Bellman equation for average reward

**Anchor Exercises**:

1. For unichain MDP with stationary policy Ï€, prove gain g^Ï€ = âˆ‘_s Ï€_s^Ï€ R_s^Ï€ where Ï€^Ï€ is stationary distribution
2. Derive Bellman equation for average reward: g + h(s) = max_a {R(s,a) + âˆ‘_s' P(s'|s,a)h(s')} where h is bias

---

## Phase VI: Bandit Algorithms (Weeks 29-33)

**Objective**: Systematic treatment of multi-armed bandits, contextual bandits, and connections to exploration-exploitation in RL.

### Week 29: Multi-Armed Bandits - Regret Framework

**Primary Reference**: Lattimore-SzepesvÃ¡ri (L-S) Ch 1-6

**Monday-Wednesday**:

- Stochastic bandits: arms, rewards, regret definition
- Lower bounds: instance-dependent and distribution-free (Lai-Robbins)
- Explore-then-commit and Îµ-greedy algorithms

**Thursday**: Prove Lai-Robbins lower bound: for any algorithm, max regret â‰¥ C log(T) for some constant C

**Friday**:

- Review: Bandit problem is "one-state MDP"; exploration-exploitation tradeoff distilled
- Code: Implement Îµ-greedy on Bernoulli bandits, plot regret vs time

**Anchor Exercises**:

1. For K-armed bandit with Gaussian arms, compute optimal Îµ(t) for Îµ-greedy to minimize regret
2. Prove regret bound for explore-then-commit: Râ‚œ = O(T^{2/3})

### Week 30: UCB Algorithms and Optimism

**Primary Reference**: L-S Ch 7-9

**Monday-Wednesday**:

- Upper Confidence Bound (UCB) algorithm
- Optimism in the face of uncertainty principle
- Chernoff-Hoeffding concentration inequalities

**Thursday**: Prove UCB regret bound: Râ‚œ â‰¤ O(âˆš(KT log T))

**Friday**:

- Review: Optimism principle generalizes to RL (optimistic value initialization)
- Code: Implement UCB, compare with Îµ-greedy on various reward distributions

**Anchor Exercises**:

1. Derive Chernoff bound: P(XÌ„â‚™ â‰¥ Î¼ + Îµ) â‰¤ exp(-2nÎµÂ²) for iid bounded random variables
2. Prove that UCB with Î±â‚œ = âˆš((2 log t)/nâ‚(t)) achieves O(log T) regret for subgaussian arms

### Week 31: Thompson Sampling and Bayesian Bandits

**Primary Reference**: L-S Ch 10-11; Russo et al. "Tutorial on Thompson Sampling"

**Monday-Wednesday**:

- Bayesian approach to bandits: prior, posterior updates
- Thompson Sampling algorithm
- Information-theoretic analysis

**Thursday**: Prove Bayesian regret bound for Thompson Sampling in Bernoulli bandit case

**Friday**:

- Review: Thompson Sampling is exploration via posterior sampling; connects to probability matching
- Code: Implement Thompson Sampling with Beta-Bernoulli conjugate prior, visualize posterior evolution

**Anchor Exercises**:

1. For Beta(Î±,Î²) prior and Bernoulli observations, derive posterior Beta(Î±+successes, Î²+failures)
2. Show Thompson Sampling is probability matching: P(arm k selected) = P(arm k is optimal | data)

### Week 32: Contextual Bandits and Linear Models

**Primary Reference**: L-S Ch 19-20

**Monday-Wednesday**:

- Contextual bandits: state information before action selection
- Linear contextual bandits: rewards are linear in features
- LinUCB algorithm and regret analysis

**Thursday**: Prove regret bound for LinUCB: Râ‚œ = O(dâˆšT log T) where d is feature dimension

**Friday**:

- Review: Contextual bandits bridge to RL (full MDP is contextual bandit with state transitions)
- Code: Implement LinUCB on synthetic contextual bandit problem

**Anchor Exercises**:

1. Derive ridge regression solution Î¸Ì‚ = (X'X + Î»I)â»Â¹X'y for linear model
2. Prove elliptical potential lemma: âˆ‘áµ— â€–xâ‚œâ€–Â²_{Vâ‚œâ»Â¹} â‰¤ 2d log(1 + t/(Î»d)) where Vâ‚œ = âˆ‘áµ¢ xáµ¢xáµ¢' + Î»I

### Week 33: Advanced Topics in Bandits

**Primary Reference**: L-S Ch 15-16 (adversarial bandits), Ch 27-28 (structured bandits)

**Monday-Wednesday**:

- Adversarial bandits and Exp3 algorithm
- Structured bandits (graph feedback, combinatorial)
- Connections to online learning and no-regret dynamics

**Thursday**: Prove Exp3 regret bound: Râ‚œ = O(âˆš(KT log K)) against adaptive adversary

**Friday**:

- Review: Adversarial bandits prepare for minimax RL (robust control)
- Code: Implement Exp3, compare with UCB under adversarial vs stochastic rewards

**Anchor Exercises**:

1. Derive multiplicative weights update: wâ‚œâ‚Šâ‚(k) = wâ‚œ(k) exp(Î·â„“â‚œ(k))
2. Prove that Exp3 with Î· = âˆš(log K/(KT)) achieves O(âˆš(KT log K)) regret

---

## Phase VII: Stochastic Approximation and RL Algorithms (Weeks 34-39)

**Objective**: Rigorous convergence theory for temporal difference learning via stochastic approximation and ODE methods.

### Week 34: Robbins-Monro and Stochastic Approximation

**Primary Reference**: Borkar "Stochastic Approximation" Ch 1-2, or Kushner-Yin Ch 1-5

**Monday-Wednesday**:

- Robbins-Monro algorithm: Î¸â‚™â‚Šâ‚ = Î¸â‚™ + Î±â‚™(h(Î¸â‚™) + Mâ‚™â‚Šâ‚)
- Step size conditions: âˆ‘Î±â‚™ = âˆ, âˆ‘Î±â‚™Â² < âˆ
- Martingale convergence theorems (Doob, martingale difference sequences)

**Thursday**: Prove Robbins-Monro convergence: if E[Mâ‚™â‚Šâ‚|â„±â‚™] = 0 and h is Lipschitz with unique root Î¸*, then Î¸â‚™ â†’ Î¸* almost surely

**Friday**:

- Review: TD learning is stochastic approximation to solve Bellman equation
- Code: Implement Robbins-Monro for finding root of simple function, vary step sizes

**Anchor Exercises**:

1. Verify that Î±â‚™ = 1/n satisfies Robbins-Monro step size conditions
2. Prove that if âˆ‘Î±â‚™Â² < âˆ and âˆ‘Î±â‚™|Mâ‚™| < âˆ a.s., then âˆ‘Î±â‚™Mâ‚™ converges a.s.

### Week 35: ODE Method for Stochastic Approximation

**Primary Reference**: Borkar Ch 3-5

**Monday-Wednesday**:

- Trajectory approximation by ODEs: Î¸Ì‡ = h(Î¸)
- Liapunov function methods for stability
- Two-timescale stochastic approximation

**Thursday**: Prove ODE method: trajectory of Î¸â‚™ tracks ODE solution, convergence follows from ODE stability

**Friday**:

- Review: ODE method is primary tool for analyzing TD convergence
- Code: Simulate SA and corresponding ODE side-by-side, observe tracking

**Anchor Exercises**:

1. For Î¸â‚™â‚Šâ‚ = Î¸â‚™ - Î±â‚™âˆ‡f(Î¸â‚™) + Î±â‚™Mâ‚™â‚Šâ‚, show that ODE is Î¸Ì‡ = -âˆ‡f(Î¸), which converges to local minimum of f
2. Analyze two-timescale SA: Î¸â‚™â‚Šâ‚ = Î¸â‚™ + Î±â‚™hÎ¸(Î¸â‚™, Ï‰â‚™), Ï‰â‚™â‚Šâ‚ = Ï‰â‚™ + Î²â‚™hÏ‰(Î¸â‚™, Ï‰â‚™) with Î²â‚™/Î±â‚™ â†’ 0

### Week 36: Temporal Difference Learning - Tabular Case

**Primary Reference**: Tsitsiklis-Van Roy, Sutton-Barto Ch 6 with rigor from SzepesvÃ¡ri "Algorithms for RL"

**Monday-Wednesday**:

- TD(0) algorithm for policy evaluation
- Fixed point characterization: TD solves projected Bellman equation
- Convergence proof via stochastic approximation

**Thursday**: Prove TD(0) converges to V^Ï€ when state space is finite

**Friday**:

- Review: TD is sample-based iterative method for solving linear system
- Code: Implement tabular TD(0) on random walk, compare with Monte Carlo

**Anchor Exercises**:

1. Show TD(0) update is Vâ‚œâ‚Šâ‚(sâ‚œ) = Vâ‚œ(sâ‚œ) + Î±â‚œ(râ‚œ + Î³Vâ‚œ(sâ‚œâ‚Šâ‚) - Vâ‚œ(sâ‚œ)) and identify as Robbins-Monro
2. Prove that for finite MDP under policy Ï€, TD converges to V^Ï€ using ODE method with ODE VÌ‡ = E[Î´Ï€(s)]

### Week 37: TD with Function Approximation - Linear Case

**Primary Reference**: Tsitsiklis-Van Roy "Analysis of Temporal-Difference Learning"; Sutton-Barto Ch 9

**Monday-Wednesday**:

- Linear function approximation: VÌ‚(s; Î¸) = Ï†(s)'Î¸
- TD with linear FA: semi-gradient update
- Projected Bellman equation and fixed point

**Thursday**: Prove convergence of linear TD to fixed point of projected Bellman operator Î â‚œğ’¯^Ï€

**Friday**:

- Review: Projection onto feature space introduces bias; fixed point â‰  V^Ï€ in general
- Code: Implement linear TD on mountain car with tile coding features

**Anchor Exercises**:

1. For linear TD, show fixed point satisfies Î¦Î¸* = Î (R + Î³PÎ¦Î¸*) where Î  projects onto span(Î¦)
2. Verify that â€–Î¸* - Î¸Ì„â€– â‰¤ (1/(1-Î³)) â€–(I-Î )V^Ï€â€– where Î¸Ì„ minimizes â€–V^Ï€ - Î¦Î¸â€– (approximation error bound)

### Week 38: Q-Learning and Off-Policy Learning

**Primary Reference**: Watkins thesis; SzepesvÃ¡ri Ch 6; Sutton-Barto Ch 6

**Monday-Wednesday**:

- Q-learning algorithm: off-policy TD control
- Convergence proof for tabular Q-learning
- Importance sampling for off-policy evaluation

**Thursday**: Prove Q-learning converges to Q* when all state-action pairs visited infinitely often

**Friday**:

- Review: Q-learning is SA solving Bellman optimality equation; off-policy enables exploration
- Code: Implement tabular Q-learning on cliff walking, compare with SARSA

**Anchor Exercises**:

1. Show Q-learning update Qâ‚œâ‚Šâ‚(s,a) = Qâ‚œ(s,a) + Î±[r + Î³ max_{a'} Qâ‚œ(s',a') - Qâ‚œ(s,a)] is SA with h(Q) = T*Q - Q
2. Prove importance sampling estimator Ï = âˆáµ— (Ï€(aáµ¢|sáµ¢)/b(aáµ¢|sáµ¢)) is unbiased for E^Ï€[G] given trajectory from b

### Week 39: Policy Gradient Methods

**Primary Reference**: Sutton et al. "Policy Gradient Methods"; Agarwal et al. "Theory of Reinforcement Learning"

**Monday-Wednesday**:

- REINFORCE algorithm: Monte Carlo policy gradient
- Actor-Critic: policy gradient + value function baseline
- Convergence analysis via two-timescale SA

**Thursday**: Prove policy gradient theorem: âˆ‡_Î¸ J(Î¸) = E^{Ï€_Î¸}[âˆ‡_Î¸ log Ï€_Î¸(a|s) Q^{Ï€_Î¸}(s,a)]

**Friday**:

- Review: Policy gradient directly optimizes performance; natural gradients use Fisher information
- Code: Implement REINFORCE on cart-pole with neural network policy

**Anchor Exercises**:

1. Derive policy gradient via likelihood ratio trick: âˆ‡_Î¸ E[f(X)] = E[f(X) âˆ‡_Î¸ log p_Î¸(X)]
2. Show that subtracting baseline b(s) from Q^Ï€(s,a) doesn't change expectation but reduces variance

---

## Phase VIII: Advanced Topics and Continuous-Time Control (Weeks 40-43)

**Objective**: Connect everything to continuous-time formulations, viscosity solutions revisited, and modern deep RL perspective.

### Week 40: Continuous-Time MDPs and HJB Revisited

**Primary Reference**: Yong-Zhou Ch 4-5; Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:

- Controlled Markov processes in continuous time
- Infinitesimal generator and Dynkin formula
- HJB equation from dynamic programming principle

**Thursday**: Prove that value function V(t,x) satisfies HJB: âˆ‚â‚œV + sup_a {ğ’œáµƒV + r(x,a)} = 0

**Friday**:

- Review: Continuous-time limit of discrete-time MDPs
- Code: Solve simple continuous-time control problem numerically (finite differences on HJB)

**Anchor Exercises**:

1. For ItÃ´ process dx = f(x,a)dt + Ïƒ(x,a)dW, compute generator ğ’œáµƒÏ† = fÂ·âˆ‡Ï† + (1/2)Tr(ÏƒÏƒ'âˆ‡Â²Ï†)
2. Derive HJB for LQR: dx = (Ax+Ba)dt, cost âˆ«(x'Qx + a'Ra)dt

### Week 41: Mean-Field Games and Multi-Agent RL

**Primary Reference**: Carmona-Delarue "Probabilistic Theory of Mean Field Games" Vol I (selected sections)

**Monday-Wednesday**:

- Mean-field limit of N-player games
- Nash equilibrium in mean-field games
- Coupled forward-backward PDEs (Fokker-Planck + HJB)

**Thursday**: Derive mean-field game system: HJB for value function + Fokker-Planck for population distribution

**Friday**:

- Review: Multi-agent RL at scale requires mean-field approximation
- Code: Simulate simple mean-field game (e.g., crowd motion), visualize Nash equilibrium

**Anchor Exercises**:

1. For linear-quadratic mean-field game, show Nash equilibrium is characterized by Riccati equation
2. Verify consistency: optimal control for representative agent matches aggregate behavior

### Week 42: Deep RL Theory - Approximation and Generalization

**Primary Reference**: Recent papers (Arora et al., Du et al. on neural tangent kernels in RL)

**Monday-Wednesday**:

- Function approximation with neural networks
- PAC bounds and sample complexity
- Neural Tangent Kernel perspective on gradient descent

**Thursday**: Survey proof sketch of sample complexity bound for fitted Q-iteration with function approximation

**Friday**:

- Review: Deep RL combines SA, FA, and SGD; theory still developing
- Code: Implement DQN on simple MDP, observe role of replay buffer and target network

**Anchor Exercises**:

1. For over-parameterized network, show training dynamics approximate kernel gradient descent in NTK regime
2. Derive Bellman error: â€–T_V - TÏ€Vâ€– â‰¥ â€–V_ - V^Ï€â€–/(1+Î³) (performance difference bound)

### Week 43: Synthesis - From Theory to Practice

**Primary Reference**: Your own notes and reflections; Survey paper of choice

**Monday-Wednesday**:

- Review key theorems: contraction mapping, ODE method, policy gradient theorem
- Survey modern algorithms: PPO, SAC, TD3 through lens of theory
- Gaps between theory and practice

**Thursday**: Write detailed summary connecting all phases: measure theory â†’ FA â†’ MDPs â†’ bandits â†’ SA â†’ deep RL

**Friday**:

- Prepare for project phase
- Code: Set up codebase for AlphaZero-lite project

**Deliverable**: 10-page synthesis document connecting mathematical foundations to algorithmic practice

---

## Phase IX: Capstone Project (Weeks 44-48)

**Objective**: Implement sophisticated RL project demonstrating mastery of theory and practice.

### Project Proposal: AlphaZero-Lite for Reversi (Othello)

**Week 44-45: Environment and MCTS Implementation**

**Monday-Wednesday**:

- Implement Reversi game engine with board representation
- Monte Carlo Tree Search from first principles
- UCT formula and exploration-exploitation

**Thursday**: Prove UCT satisfies "regret minimization" property (relates to bandit theory from Phase VI)

**Friday**: Integrate and test MCTS on random play

**Deliverable**: Fully functional MCTS player that defeats random play

### Week 46-47: Neural Network Policy-Value Architecture**

**Monday-Wednesday**:

- Design convolutional architecture for board evaluation
- Policy head (action probabilities) and value head (position evaluation)
- Self-play data generation pipeline

**Thursday**: Implement training loop with mini-batch SGD

**Friday**: Test trained network as MCTS evaluation function

**Deliverable**: Trained network that improves MCTS play strength

### Week 48: Ablation Studies and Final Analysis**

**Monday-Wednesday**:

- Ablation: MCTS depth, network architecture, training iterations
- Compare AlphaZero-lite vs pure MCTS vs pure network
- ELO rating estimation

**Thursday**: Write detailed technical report connecting to theory:

- MCTS as UCB on tree (bandit theory)
- Value function approximation (Sobolev regularity of learned V)
- Policy improvement theorem (MDP theory)

**Friday**: Final presentation preparation

**Final Deliverable**:

1. Complete codebase on GitHub
2. 15-page technical report
3. Trained model checkpoint and evaluation results

---

## Daily Reflection Protocol

Each Friday, maintain a **learning log** with three components:

1. **Mathematical Insight** (3-5 sentences): What theorem or proof technique was most illuminating this week? Why?
    
2. **RL Connection** (2-3 sentences): How does this week's mathematics manifest in reinforcement learning algorithms?
    
3. **Open Questions** (1-2 questions): What remains unclear or invites deeper investigation?
    

This log becomes appendix material for your final synthesis document.

---

## Reference Library Organization

**Tier 1 (weekly use)**:

- Folland "Real Analysis"
- Brezis "Functional Analysis, Sobolev Spaces and PDEs"
- Puterman "Markov Decision Processes"
- Lattimore-SzepesvÃ¡ri "Bandit Algorithms"

**Tier 2 (regular reference)**:

- Durrett "Probability: Theory and Examples"
- Levin-Peres-Wilmer "Markov Chains and Mixing Times"
- Borkar "Stochastic Approximation"
- Bertsekas "Reinforcement Learning and Optimal Control"

**Tier 3 (specialized topics)**:

- Meyn-Tweedie "Markov Chains and Stochastic Stability"
- Yong-Zhou "Stochastic Controls"
- Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions"
- Sutton-Barto "Reinforcement Learning" (narrative guide)

---
