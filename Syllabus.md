# A 48-Week Rigorous Syllabus: From Measure Theory to Reinforcement Learning

## Structural Principles

**Daily Time Allocation**: 90 minutes, Monday-Friday (weekends off)

**Adaptive Weekly Template**:

- **Monday-Wednesday**: 40 min reading + 40 min proof/exercise + 10 min reflection note
- **Thursday**: 30 min reading + 60 min extended proof or multi-part exercise
- **Friday**: 20 min reading + 30 min proof review + 40 min coding synthesis

**Phase Structure**: Six major phases with clear mathematical and RL objectives

---

## Phase I: Measure-Theoretic Foundations (Weeks 1-6)

**Objective**: Rapid but rigorous traversal of measure theory, emphasizing aspects critical for probability and RL. Given your background, this serves as structured review with targeted depth.

### Week 1: σ-Algebras, Measures, and Measurable Functions

**Primary Reference**: Folland §1.1-1.4

**Monday-Wednesday**:

- σ-algebras and their generation (monotone classes, Dynkin's π-λ theorem)
- Measures, outer measures, and Carathéodory extension theorem
- Measurable functions and their arithmetic/composition properties

**Thursday**: Extended proof of Carathéodory extension theorem (construction of Lebesgue measure from outer measure on rectangles)

**Friday**:

- Review: Why σ-algebras encode observability in RL
- Code: Implement discrete σ-algebra generator, visualize set operations

**Anchor Exercises**:

1. Prove the π-λ theorem: if 𝒫 is a π-system and ℒ is a λ-system with 𝒫 ⊆ ℒ, then σ(𝒫) ⊆ ℒ
2. Show that the Borel σ-algebra on ℝⁿ is generated by open balls (or by half-open rectangles)

### Week 2: Integration and Convergence Theorems

**Primary Reference**: Folland §2.1-2.4

**Monday-Wednesday**:

- Construction of the Lebesgue integral (simple functions → non-negative → general)
- Monotone Convergence Theorem (MCT)
- Fatou's Lemma and Dominated Convergence Theorem (DCT)

**Thursday**: Complete proof of DCT from first principles, emphasizing the role of dominating function

**Friday**:

- Review: How DCT enables interchange of expectation and limits in RL (e.g., policy evaluation)
- Code: Numerical experiments showing failure of pointwise limit interchange without domination

**Anchor Exercises**:

1. Prove that if fₙ → f in L¹, there exists a subsequence converging almost everywhere
2. Construct explicit counterexamples showing necessity of each hypothesis in MCT and DCT

### Week 3: Product Measures and Fubini's Theorem

**Primary Reference**: Folland §2.4-2.5, Durrett §1.7

**Monday-Wednesday**:

- Product σ-algebras and product measures
- Fubini-Tonelli theorem (complete statement and proof strategy)
- Applications to transition kernels in MDPs

**Thursday**: Full proof of Fubini's theorem for σ-finite measures

**Friday**:

- Review: Fubini justifies swapping ∑ₛ′ ∫ₐ in Bellman operator
- Code: Monte Carlo integration in 2D demonstrating Fubini, compare with iterated integration

**Anchor Exercises**:

1. Prove the Tonelli form: if f ≥ 0 is measurable on X × Y, then ∫(∫f dμ)dν = ∫(∫f dν)dμ
2. Show σ-finiteness is necessary: construct counterexample with counting measure on uncountable set

### Week 4: Radon-Nikodym and Signed Measures

**Primary Reference**: Folland §3.1-3.4

**Monday-Wednesday**:

- Signed measures and Hahn decomposition
- Absolute continuity and singularity of measures
- Radon-Nikodym theorem: existence of density dμ/dν

**Thursday**: Complete proof of Radon-Nikodym theorem via maximal element in Hilbert space

**Friday**:

- Review: Radon-Nikodym underlies importance sampling and likelihood ratios in RL
- Code: Implement Radon-Nikodym density estimation numerically for simple examples

**Anchor Exercises**:

1. Prove the Hahn decomposition theorem for signed measures
2. Show that Radon-Nikodym derivative satisfies chain rule: if μ ≪ ν ≪ λ, then d(μ/λ) = (dμ/dν)(dν/λ) a.e.

### Week 5: Lᵖ Spaces and Duality

**Primary Reference**: Folland §6.1-6.3

**Monday-Wednesday**:

- Definition of Lᵖ(μ) spaces, Hölder and Minkowski inequalities
- Completeness of Lᵖ (Riesz-Fischer theorem)
- Duality: (Lᵖ)* ≅ Lq for p ∈ [1,∞), 1/p + 1/q = 1

**Thursday**: Prove Riesz representation theorem for (Lᵖ)* when 1 < p < ∞

**Friday**:

- Review: Value functions live in L∞, policy gradients involve L² inner products
- Code: Visualize convergence in different Lᵖ norms, demonstrate non-convergence in one norm vs another

**Anchor Exercises**:

1. Prove Hölder's inequality via convexity (Young's inequality for products)
2. Show that L∞ is not separable, while Lᵖ for p < ∞ is separable (on σ-finite measure spaces)

### Week 6: Probability Spaces and Conditional Expectation

**Primary Reference**: Durrett §1.1-1.6, §5.1

**Monday-Wednesday**:

- Probability spaces, random variables, and distributions
- Independence and product measures
- Conditional expectation as orthogonal projection in L²
- Properties: tower property, taking out what is known

**Thursday**: Prove existence and uniqueness of conditional expectation using Hilbert space projection

**Friday**:

- Review: E[R|s,a] in MDPs is conditional expectation; tower property gives Bellman consistency
- Code: Simulate conditional expectation numerically, verify tower property empirically

**Anchor Exercises**:

1. Prove that E[X|𝒢] minimizes E[(X - Y)²] over all 𝒢-measurable Y
2. Establish the tower property: E[E[X|𝒢]|ℋ] = E[X|ℋ] when ℋ ⊆ 𝒢

---

## Phase II: Markov Chains and Ergodic Theory (Weeks 7-12)

**Objective**: Master discrete-time Markov chains on finite and countable spaces, establishing the probabilistic backbone of MDPs.

### Week 7: Finite Markov Chains - Fundamentals

**Primary Reference**: Levin-Peres-Wilmer (LPW) Ch 1-4

**Monday-Wednesday**:

- Transition matrices and Chapman-Kolmogorov equation
- Irreducibility, aperiodicity, and communication classes
- Stationary distributions and detailed balance

**Thursday**: Prove existence and uniqueness of stationary distribution for irreducible finite chains

**Friday**:

- Review: Stationary distribution is "average" state visitation under policy
- Code: Implement random walk on graphs, compute stationary distribution via eigenvalue and via simulation

**Anchor Exercises**:

1. Prove that irreducible finite chain has unique stationary distribution π with πᵢ > 0 for all i
2. Show detailed balance (πᵢPᵢⱼ = πⱼPⱼᵢ) implies π is stationary

### Week 8: Convergence and Mixing Times

**Primary Reference**: LPW Ch 5-7, 12

**Monday-Wednesday**:

- Total variation distance and coupling
- Convergence to stationarity: coupling lemma
- Mixing time definition and examples

**Thursday**: Prove coupling inequality: d(t) ≤ P(X₁(t) ≠ X₂(t)) for chains started from coupling (X₁,X₂)

**Friday**:

- Review: Mixing time bounds sample complexity in RL exploration
- Code: Implement coupling for simple chains, visualize convergence to stationarity in TV norm

**Anchor Exercises**:

1. Compute mixing time for random walk on cycle graph Cₙ
2. Prove that for reversible chains, τ_mix ≤ (λ₂*)⁻¹ log(1/(πₘᵢₙδ)) where λ₂* = 1 - λ₂ is spectral gap

### Week 9: Markov Chain Monte Carlo

**Primary Reference**: LPW Ch 8-9, 13-14

**Monday-Wednesday**:

- Metropolis-Hastings algorithm and its variants
- Gibbs sampling
- Path coupling method for bounding mixing

**Thursday**: Prove correctness of Metropolis-Hastings: constructed chain has desired stationary distribution

**Friday**:

- Review: MCMC samples from complex distributions (e.g., Boltzmann exploration in RL)
- Code: Implement Metropolis-Hastings for 2D Ising model, verify convergence to Gibbs measure

**Anchor Exercises**:

1. Design Metropolis-Hastings algorithm to sample from π(x) ∝ exp(-U(x)) on finite state space
2. Prove detailed balance for Gibbs sampler on product space

### Week 10: Countable State Spaces - Transience and Recurrence

**Primary Reference**: Durrett §6.1-6.4

**Monday-Wednesday**:

- Classification: transient vs recurrent states
- Null recurrence vs positive recurrence
- First passage times and hitting probabilities

**Thursday**: Prove Pólya's theorem: random walk on ℤᵈ is recurrent iff d ≤ 2

**Friday**:

- Review: Recurrence structure of state space affects exploration in RL
- Code: Simulate random walks in dimensions 1,2,3; empirically verify recurrence/transience

**Anchor Exercises**:

1. Show that P(return to 0|start at 0) = ∑ₙ P₀₀⁽ⁿ⁾ / (1 + ∑ₙ P₀₀⁽ⁿ⁾)
2. Prove that in finite state space, not all states can be transient

### Week 11: Ergodic Theorems

**Primary Reference**: Durrett §6.5-6.7, Meyn-Tweedie Ch 17

**Monday-Wednesday**:

- Strong law for Markov chains
- Birkhoff's ergodic theorem (statement for measure-preserving transformations)
- Application to time averages in Markov chains

**Thursday**: Prove Birkhoff ergodic theorem for finite state space via subadditive ergodic theorem

**Friday**:

- Review: Ergodic theorem justifies Monte Carlo policy evaluation (time average = ensemble average)
- Code: Verify ergodic theorem numerically for various chains; demonstrate failure for non-ergodic chain

**Anchor Exercises**:

1. For irreducible positive recurrent chain, prove (1/n)∑ᵢ₌₁ⁿ f(Xᵢ) → ∑ₓ πₓf(x) almost surely
2. Show necessity of positive recurrence: construct null recurrent chain where time average exists but differs from spatial average

### Week 12: General State Spaces - Introduction

**Primary Reference**: Meyn-Tweedie Ch 3-4 (selected topics)

**Monday-Wednesday**:

- Transition kernels on general measurable spaces
- Feller property and weak convergence
- Irreducibility and aperiodicity in general spaces

**Thursday**: Prove that Feller property implies weak continuity in initial distribution

**Friday**:

- Review: Continuous state RL requires general state space theory
- Code: Implement Markov chain on [0,1] with continuous kernel, visualize evolution of density

**Anchor Exercises**:

1. Define φ-irreducibility and show every finite chain is φ-irreducible for counting measure φ
2. Verify Feller property for Gaussian random walk: Xₙ₊₁ = Xₙ + εₙ with εₙ ~ N(0,σ²)

---

## Phase III: Functional Analysis and Operator Theory (Weeks 13-18)

**Objective**: Establish the function space framework where Bellman operators and value functions reside. Deep dive into Banach and Hilbert spaces.

### Week 13: Normed Spaces and Banach Spaces

**Primary Reference**: Brezis Ch 1, §1.1-1.3

**Monday-Wednesday**:

- Definition and examples of normed vector spaces
- Completeness and Banach spaces
- Finite-dimensional spaces: equivalence of norms

**Thursday**: Prove all norms on ℝⁿ are equivalent (any two induce same topology)

**Friday**:

- Review: Space of bounded functions ℬ(𝒮) with supremum norm is Banach; value iteration works here
- Code: Visualize unit balls in ℝ² for various norms (L¹, L², L∞)

**Anchor Exercises**:

1. Show C([0,1]) with supremum norm is complete but C([0,1]) with L¹ norm is not
2. Prove that every finite-dimensional normed space is complete

### Week 14: Linear Operators and Dual Spaces

**Primary Reference**: Brezis Ch 1, §1.4-1.6

**Monday-Wednesday**:

- Bounded linear operators, operator norm
- Dual space and Hahn-Banach theorem (extension form)
- Weak and weak* topologies

**Thursday**: Prove Hahn-Banach theorem (geometric form: separating hyperplane)

**Friday**:

- Review: Linear value functions correspond to functionals; duality essential for policy gradients
- Code: Implement linear functionals on finite-dimensional spaces, visualize separating hyperplanes

**Anchor Exercises**:

1. If X is Banach and T:X → Y linear with closed graph, prove T is bounded (closed graph theorem)
2. Compute dual of c₀ (sequences converging to 0) and show (c₀)* ≅ ℓ¹

### Week 15: Three Fundamental Theorems

**Primary Reference**: Brezis Ch 2, §2.1-2.7

**Monday-Wednesday**:

- Uniform Boundedness Principle (Banach-Steinhaus)
- Open Mapping Theorem and Inverse Mapping Theorem
- Closed Graph Theorem

**Thursday**: Prove Uniform Boundedness Principle via Baire category theorem

**Friday**:

- Review: UBP implies pointwise bounded sequence of Bellman backups is uniformly bounded
- Code: Construct example of pointwise bounded but not uniformly bounded sequence on infinite-dimensional space

**Anchor Exercises**:

1. Use UBP to show: if {Tₙ} ⊂ ℒ(X,Y) and Tₙx converges for each x, then T := lim Tₙ is bounded
2. Prove that surjective bounded operator between Banach spaces is open

### Week 16: Hilbert Spaces

**Primary Reference**: Brezis Ch 5, §5.1-5.5

**Monday-Wednesday**:

- Inner product spaces and Cauchy-Schwarz inequality
- Orthogonality and projection theorem
- Riesz representation: H* ≅ H

**Thursday**: Prove projection theorem: every closed convex subset of Hilbert space contains unique nearest point to any given point

**Friday**:

- Review: Least-squares TD uses orthogonal projection in feature space (Hilbert space)
- Code: Implement orthogonal projection onto subspaces in ℝⁿ, visualize geometry

**Anchor Exercises**:

1. Prove Parallelogram law: ‖x+y‖² + ‖x-y‖² = 2(‖x‖² + ‖y‖²) characterizes inner product spaces
2. Show that weak convergence xₙ ⇀ x and ‖xₙ‖ → ‖x‖ together imply strong convergence xₙ → x

### Week 17: Compact Operators and Spectral Theory

**Primary Reference**: Brezis Ch 6, §6.1-6.5

**Monday-Wednesday**:

- Compact operators and Fredholm alternative
- Spectrum of bounded operator
- Spectral theorem for compact self-adjoint operators

**Thursday**: Prove spectral theorem: compact self-adjoint T on Hilbert space H has orthonormal eigenbasis

**Friday**:

- Review: Compactness of certain Bellman operators enables finite-rank approximation
- Code: Compute spectrum of discrete Laplacian, verify eigenfunction expansion

**Anchor Exercises**:

1. Show composition of bounded operator with compact operator is compact
2. For compact self-adjoint T, prove Tx = λx with λ ≠ 0 iff λ is eigenvalue

### Week 18: Contraction Mappings and Fixed Points

**Primary Reference**: Brezis Ch 9, §9.1-9.3; Puterman Appendix A

**Monday-Wednesday**:

- Metric spaces and completeness
- Banach Fixed Point Theorem (Contraction Mapping Theorem)
- Applications to differential and integral equations

**Thursday**: Prove Banach Fixed Point Theorem with explicit rate of convergence

**Friday**:

- Review: Bellman operator T^π is γ-contraction in L∞, yielding unique fixed point V^π
- Code: Visualize fixed point iteration for various contractions, demonstrate linear convergence

**Anchor Exercises**:

1. Prove that if T:X → X is contraction with fixed point x*, then d(Tⁿx₀, x*) ≤ Lⁿd(x₀,x*)/(1-L)
2. Show non-expansive map on compact metric space has fixed point (Brouwer via approximation)

---

## Phase IV: Sobolev Spaces, PDEs, and Control Theory (Weeks 19-24)

**Objective**: Deep exploration of Sobolev spaces and variational methods, connecting to Hamilton-Jacobi-Bellman equations and continuous control. This is where your PDE background shines.

### Week 19: Weak Derivatives and Sobolev Spaces W^{k,p}

**Primary Reference**: Brezis Ch 8, §8.1-8.3

**Monday-Wednesday**:

- Weak derivatives: definition via integration by parts
- Sobolev spaces W^{k,p}(Ω) and their basic properties
- Relationship between classical and weak solutions

**Thursday**: Prove that W^{1,p}(Ω) with ‖u‖ = (‖u‖ₚᵖ + ‖∇u‖ₚᵖ)^{1/p} is Banach space

**Friday**:

- Review: Neural networks produce W^{1,∞} functions (Lipschitz); Sobolev regularity affects approximation quality
- Code: Compute weak derivatives numerically via finite differences, compare with classical derivatives

**Anchor Exercises**:

1. Show that if u ∈ W^{1,p}(Ω) and Ω is bounded Lipschitz, then γu (trace) is well-defined in L^p(∂Ω)
2. Prove density of C^∞ₖ(Ω) in W^{1,p}(Ω) when Ω has smooth boundary

### Week 20: Sobolev Embeddings and Compactness

**Primary Reference**: Brezis Ch 9, §9.1-9.4

**Monday-Wednesday**:

- Sobolev embedding theorem: W^{1,p}(ℝⁿ) ↪ L^q(ℝⁿ) for appropriate q
- Rellich-Kondrachov compact embedding theorem
- Poincaré and Poincaré-Wirtinger inequalities

**Thursday**: Prove Gagliardo-Nirenberg-Sobolev inequality: if p < n, then W^{1,p}(ℝⁿ) ↪ L^{p*}(ℝⁿ) where 1/p* = 1/p - 1/n

**Friday**:

- Review: Sobolev embeddings ensure policies in W^{1,p} have bounded supremum norm (needed for stability)
- Code: Numerically verify Sobolev embeddings for functions on unit interval

**Anchor Exercises**:

1. Prove Poincaré inequality on bounded connected Ω: ‖u - ū‖_{L²} ≤ C‖∇u‖_{L²} where ū = mean of u
2. Show Rellich theorem implies: bounded sequence in W^{1,p}(Ω) has L^p-convergent subsequence (when p < n*)

### Week 21: Variational Formulations and Weak Solutions

**Primary Reference**: Brezis Ch 8, §8.4-8.6; Ch 9, §9.5

**Monday-Wednesday**:

- Lax-Milgram theorem and weak solutions to elliptic PDEs
- Energy minimization and Euler-Lagrange equations
- Regularity theory: weak solutions are stronger

**Thursday**: Prove Lax-Milgram theorem: coercive continuous bilinear form on Hilbert space gives unique weak solution to Lu = f

**Friday**:

- Review: Value function in continuous control solves variational problem; weak formulation enables numerical methods
- Code: Solve Poisson equation -Δu = f using finite elements (weak formulation)

**Anchor Exercises**:

1. For -Δu + u = f in Ω with u = 0 on ∂Ω, write weak formulation and prove existence/uniqueness via Lax-Milgram
2. Show that weak solution in W^{1,2}_0(Ω) of -Δu = f ∈ L²(Ω) actually belongs to W^{2,2}(Ω) (H² regularity)

### Week 22: Hamilton-Jacobi Equations and Viscosity Solutions

**Primary Reference**: Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions" Ch 1-2 (or similar reference)

**Monday-Wednesday**:

- First-order Hamilton-Jacobi equations
- Characteristics and method of characteristics (when it works)
- Need for weak solutions: shocks and non-uniqueness

**Thursday**: Derive Hamilton-Jacobi-Bellman equation from dynamic programming principle in continuous time

**Friday**:

- Review: HJB equation is PDE satisfied by value function; viscosity solutions handle non-smoothness
- Code: Solve HJ equation ∂ₜu + H(∇u) = 0 numerically via upwind scheme, observe shock formation

**Anchor Exercises**:

1. For u_t + H(u_x) = 0 with H convex, derive Rankine-Hugoniot condition for shock propagation
2. Show that smooth solution to HJB equation gives optimal control via feedback law a*(s) = arg max H(s, ∇V(s), a)

### Week 23: Viscosity Solutions - Theory

**Primary Reference**: Bardi-Capuzzo Dolcetta Ch 3-4; or Crandall-Ishii-Lions (User's Guide)

**Monday-Wednesday**:

- Definition of viscosity sub/super solutions
- Comparison principle for viscosity solutions
- Existence and uniqueness theorems

**Thursday**: Prove comparison principle for viscosity solutions of Hamilton-Jacobi equation (simplified case)

**Friday**:

- Review: Viscosity solution is "right" notion for value function; DQN approximates viscosity solution
- Code: Implement Godunov scheme for Hamilton-Jacobi, verify convergence to viscosity solution

**Anchor Exercises**:

1. Verify that u(x,t) = min_{y} {|x-y|²/(2t) + u₀(y)} is viscosity solution to heat equation for smooth u₀
2. Prove uniqueness of viscosity solution to u + H(∇u) = 0 when H is coercive

### Week 24: Optimal Control and HJB in Continuous Time

**Primary Reference**: Yong-Zhou "Stochastic Controls" Ch 3-4, or Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:

- Controlled SDEs and diffusions
- Dynamic programming in continuous time
- HJB equation for stochastic control: ρV - 𝒜V = max_a {r(s,a)} where 𝒜 is infinitesimal generator

**Thursday**: Derive HJB equation for stochastic control problem with Brownian noise via Itô's formula

**Friday**:

- Review: Actor-Critic algorithms solve HJB numerically; connection to policy gradient
- Code: Solve simple LQR problem analytically via HJB, compare with Riccati equation solution

**Anchor Exercises**:

1. For linear-quadratic regulator dx = (Ax + Ba)dt + σdW with cost ∫(x'Qx + a'Ra)dt, derive Riccati equation from HJB
2. Verify verification theorem: if V solves HJB with feedback control a*(s), then V is value function

---

## Phase V: Markov Decision Processes and Dynamic Programming (Weeks 25-28)

**Objective**: Rigorous MDP theory where all prior mathematics pays dividends. Existence, uniqueness, convergence.

### Week 25: MDP Formalism and Bellman Equations

**Primary Reference**: Puterman Ch 3-4, Bertsekas Vol I Ch 1-2

**Monday-Wednesday**:

- Formal definition: (𝒮, 𝒜, P, R, γ) with measurability requirements
- Policies (deterministic, stationary, history-dependent)
- Value functions V^π and Q^π; Bellman consistency equations
- Bellman optimality equation

**Thursday**: Prove that V^π is unique fixed point of Bellman operator T^πV = R^π + γP^πV

**Friday**:

- Review: Everything from measure theory to functional analysis culminates here
- Code: Implement gridworld MDP, verify Bellman equations numerically

**Anchor Exercises**:

1. Prove T^π is γ-contraction in (ℬ(𝒮), ‖·‖∞): ‖T^πV - T^πW‖∞ ≤ γ‖V - W‖∞
2. Show that Q^π satisfies Q^π(s,a) = R(s,a) + γ∑_{s'} P(s'|s,a) ∑_a' π(a'|s')Q^π(s',a')

### Week 26: Optimal Policies and Value Iteration

**Primary Reference**: Puterman Ch 6, Bertsekas Vol I Ch 1

**Monday-Wednesday**:

- Existence of optimal policies for discounted infinite-horizon MDPs
- Value iteration algorithm and convergence analysis
- Optimality equations and policy improvement

**Thursday**: Prove convergence of value iteration: Vₙ → V* at rate ‖Vₙ - V*‖ ≤ γⁿ‖V₀ - V*‖/(1-γ)

**Friday**:

- Review: Contraction mapping theorem is why value iteration works
- Code: Implement value iteration on various MDPs, plot convergence in sup norm

**Anchor Exercises**:

1. Prove policy improvement theorem: if Q^π(s, π'(s)) ≥ V^π(s) for all s, then V^{π'} ≥ V^π
2. Show that greedy policy with respect to V* is optimal: π*(s) ∈ arg max_a Q*(s,a)

### Week 27: Policy Iteration and Linear Programming

**Primary Reference**: Puterman Ch 6-7

**Monday-Wednesday**:

- Policy iteration algorithm: evaluation + improvement
- Convergence in finite steps for finite MDPs
- Linear programming formulation of MDP

**Thursday**: Prove policy iteration converges in finite steps when |𝒮| × |𝒜| < ∞

**Friday**:

- Review: Policy iteration is Newton's method on Bellman optimality
- Code: Implement policy iteration, compare with value iteration on computational efficiency

**Anchor Exercises**:

1. Prove that policy evaluation via solving (I - γP^π)V = R^π gives V^π
2. Formulate MDP as LP: maximize ∑_s ν(s)V(s) subject to V(s) ≥ R(s,a) + γ∑_s' P(s'|s,a)V(s') for all s,a

### Week 28: Average Reward MDPs and Ergodic Theory

**Primary Reference**: Puterman Ch 8, Meyn-Tweedie Ch 17

**Monday-Wednesday**:

- Average reward criterion: lim (1/n)E[∑ᵢ₌₁ⁿ R(sᵢ,aᵢ)]
- Existence of bias and gain
- Connection to ergodic theorems for Markov chains

**Thursday**: Prove that for unichain MDP, average reward equals (stationary distribution)·(reward vector)

**Friday**:

- Review: Average reward relevant for continuing tasks (no discounting)
- Code: Compute average reward via simulation, compare with solving Bellman equation for average reward

**Anchor Exercises**:

1. For unichain MDP with stationary policy π, prove gain g^π = ∑_s π_s^π R_s^π where π^π is stationary distribution
2. Derive Bellman equation for average reward: g + h(s) = max_a {R(s,a) + ∑_s' P(s'|s,a)h(s')} where h is bias

---

## Phase VI: Bandit Algorithms (Weeks 29-33)

**Objective**: Systematic treatment of multi-armed bandits, contextual bandits, and connections to exploration-exploitation in RL.

### Week 29: Multi-Armed Bandits - Regret Framework

**Primary Reference**: Lattimore-Szepesvári (L-S) Ch 1-6

**Monday-Wednesday**:

- Stochastic bandits: arms, rewards, regret definition
- Lower bounds: instance-dependent and distribution-free (Lai-Robbins)
- Explore-then-commit and ε-greedy algorithms

**Thursday**: Prove Lai-Robbins lower bound: for any algorithm, max regret ≥ C log(T) for some constant C

**Friday**:

- Review: Bandit problem is "one-state MDP"; exploration-exploitation tradeoff distilled
- Code: Implement ε-greedy on Bernoulli bandits, plot regret vs time

**Anchor Exercises**:

1. For K-armed bandit with Gaussian arms, compute optimal ε(t) for ε-greedy to minimize regret
2. Prove regret bound for explore-then-commit: Rₜ = O(T^{2/3})

### Week 30: UCB Algorithms and Optimism

**Primary Reference**: L-S Ch 7-9

**Monday-Wednesday**:

- Upper Confidence Bound (UCB) algorithm
- Optimism in the face of uncertainty principle
- Chernoff-Hoeffding concentration inequalities

**Thursday**: Prove UCB regret bound: Rₜ ≤ O(√(KT log T))

**Friday**:

- Review: Optimism principle generalizes to RL (optimistic value initialization)
- Code: Implement UCB, compare with ε-greedy on various reward distributions

**Anchor Exercises**:

1. Derive Chernoff bound: P(X̄ₙ ≥ μ + ε) ≤ exp(-2nε²) for iid bounded random variables
2. Prove that UCB with αₜ = √((2 log t)/nₐ(t)) achieves O(log T) regret for subgaussian arms

### Week 31: Thompson Sampling and Bayesian Bandits

**Primary Reference**: L-S Ch 10-11; Russo et al. "Tutorial on Thompson Sampling"

**Monday-Wednesday**:

- Bayesian approach to bandits: prior, posterior updates
- Thompson Sampling algorithm
- Information-theoretic analysis

**Thursday**: Prove Bayesian regret bound for Thompson Sampling in Bernoulli bandit case

**Friday**:

- Review: Thompson Sampling is exploration via posterior sampling; connects to probability matching
- Code: Implement Thompson Sampling with Beta-Bernoulli conjugate prior, visualize posterior evolution

**Anchor Exercises**:

1. For Beta(α,β) prior and Bernoulli observations, derive posterior Beta(α+successes, β+failures)
2. Show Thompson Sampling is probability matching: P(arm k selected) = P(arm k is optimal | data)

### Week 32: Contextual Bandits and Linear Models

**Primary Reference**: L-S Ch 19-20

**Monday-Wednesday**:

- Contextual bandits: state information before action selection
- Linear contextual bandits: rewards are linear in features
- LinUCB algorithm and regret analysis

**Thursday**: Prove regret bound for LinUCB: Rₜ = O(d√T log T) where d is feature dimension

**Friday**:

- Review: Contextual bandits bridge to RL (full MDP is contextual bandit with state transitions)
- Code: Implement LinUCB on synthetic contextual bandit problem

**Anchor Exercises**:

1. Derive ridge regression solution θ̂ = (X'X + λI)⁻¹X'y for linear model
2. Prove elliptical potential lemma: ∑ᵗ ‖xₜ‖²_{Vₜ⁻¹} ≤ 2d log(1 + t/(λd)) where Vₜ = ∑ᵢ xᵢxᵢ' + λI

### Week 33: Advanced Topics in Bandits

**Primary Reference**: L-S Ch 15-16 (adversarial bandits), Ch 27-28 (structured bandits)

**Monday-Wednesday**:

- Adversarial bandits and Exp3 algorithm
- Structured bandits (graph feedback, combinatorial)
- Connections to online learning and no-regret dynamics

**Thursday**: Prove Exp3 regret bound: Rₜ = O(√(KT log K)) against adaptive adversary

**Friday**:

- Review: Adversarial bandits prepare for minimax RL (robust control)
- Code: Implement Exp3, compare with UCB under adversarial vs stochastic rewards

**Anchor Exercises**:

1. Derive multiplicative weights update: wₜ₊₁(k) = wₜ(k) exp(ηℓₜ(k))
2. Prove that Exp3 with η = √(log K/(KT)) achieves O(√(KT log K)) regret

---

## Phase VII: Stochastic Approximation and RL Algorithms (Weeks 34-39)

**Objective**: Rigorous convergence theory for temporal difference learning via stochastic approximation and ODE methods.

### Week 34: Robbins-Monro and Stochastic Approximation

**Primary Reference**: Borkar "Stochastic Approximation" Ch 1-2, or Kushner-Yin Ch 1-5

**Monday-Wednesday**:

- Robbins-Monro algorithm: θₙ₊₁ = θₙ + αₙ(h(θₙ) + Mₙ₊₁)
- Step size conditions: ∑αₙ = ∞, ∑αₙ² < ∞
- Martingale convergence theorems (Doob, martingale difference sequences)

**Thursday**: Prove Robbins-Monro convergence: if E[Mₙ₊₁|ℱₙ] = 0 and h is Lipschitz with unique root θ*, then θₙ → θ* almost surely

**Friday**:

- Review: TD learning is stochastic approximation to solve Bellman equation
- Code: Implement Robbins-Monro for finding root of simple function, vary step sizes

**Anchor Exercises**:

1. Verify that αₙ = 1/n satisfies Robbins-Monro step size conditions
2. Prove that if ∑αₙ² < ∞ and ∑αₙ|Mₙ| < ∞ a.s., then ∑αₙMₙ converges a.s.

### Week 35: ODE Method for Stochastic Approximation

**Primary Reference**: Borkar Ch 3-5

**Monday-Wednesday**:

- Trajectory approximation by ODEs: θ̇ = h(θ)
- Liapunov function methods for stability
- Two-timescale stochastic approximation

**Thursday**: Prove ODE method: trajectory of θₙ tracks ODE solution, convergence follows from ODE stability

**Friday**:

- Review: ODE method is primary tool for analyzing TD convergence
- Code: Simulate SA and corresponding ODE side-by-side, observe tracking

**Anchor Exercises**:

1. For θₙ₊₁ = θₙ - αₙ∇f(θₙ) + αₙMₙ₊₁, show that ODE is θ̇ = -∇f(θ), which converges to local minimum of f
2. Analyze two-timescale SA: θₙ₊₁ = θₙ + αₙhθ(θₙ, ωₙ), ωₙ₊₁ = ωₙ + βₙhω(θₙ, ωₙ) with βₙ/αₙ → 0

### Week 36: Temporal Difference Learning - Tabular Case

**Primary Reference**: Tsitsiklis-Van Roy, Sutton-Barto Ch 6 with rigor from Szepesvári "Algorithms for RL"

**Monday-Wednesday**:

- TD(0) algorithm for policy evaluation
- Fixed point characterization: TD solves projected Bellman equation
- Convergence proof via stochastic approximation

**Thursday**: Prove TD(0) converges to V^π when state space is finite

**Friday**:

- Review: TD is sample-based iterative method for solving linear system
- Code: Implement tabular TD(0) on random walk, compare with Monte Carlo

**Anchor Exercises**:

1. Show TD(0) update is Vₜ₊₁(sₜ) = Vₜ(sₜ) + αₜ(rₜ + γVₜ(sₜ₊₁) - Vₜ(sₜ)) and identify as Robbins-Monro
2. Prove that for finite MDP under policy π, TD converges to V^π using ODE method with ODE V̇ = E[δπ(s)]

### Week 37: TD with Function Approximation - Linear Case

**Primary Reference**: Tsitsiklis-Van Roy "Analysis of Temporal-Difference Learning"; Sutton-Barto Ch 9

**Monday-Wednesday**:

- Linear function approximation: V̂(s; θ) = φ(s)'θ
- TD with linear FA: semi-gradient update
- Projected Bellman equation and fixed point

**Thursday**: Prove convergence of linear TD to fixed point of projected Bellman operator Πₜ𝒯^π

**Friday**:

- Review: Projection onto feature space introduces bias; fixed point ≠ V^π in general
- Code: Implement linear TD on mountain car with tile coding features

**Anchor Exercises**:

1. For linear TD, show fixed point satisfies Φθ* = Π(R + γPΦθ*) where Π projects onto span(Φ)
2. Verify that ‖θ* - θ̄‖ ≤ (1/(1-γ)) ‖(I-Π)V^π‖ where θ̄ minimizes ‖V^π - Φθ‖ (approximation error bound)

### Week 38: Q-Learning and Off-Policy Learning

**Primary Reference**: Watkins thesis; Szepesvári Ch 6; Sutton-Barto Ch 6

**Monday-Wednesday**:

- Q-learning algorithm: off-policy TD control
- Convergence proof for tabular Q-learning
- Importance sampling for off-policy evaluation

**Thursday**: Prove Q-learning converges to Q* when all state-action pairs visited infinitely often

**Friday**:

- Review: Q-learning is SA solving Bellman optimality equation; off-policy enables exploration
- Code: Implement tabular Q-learning on cliff walking, compare with SARSA

**Anchor Exercises**:

1. Show Q-learning update Qₜ₊₁(s,a) = Qₜ(s,a) + α[r + γ max_{a'} Qₜ(s',a') - Qₜ(s,a)] is SA with h(Q) = T*Q - Q
2. Prove importance sampling estimator ρ = ∏ᵗ (π(aᵢ|sᵢ)/b(aᵢ|sᵢ)) is unbiased for E^π[G] given trajectory from b

### Week 39: Policy Gradient Methods

**Primary Reference**: Sutton et al. "Policy Gradient Methods"; Agarwal et al. "Theory of Reinforcement Learning"

**Monday-Wednesday**:

- REINFORCE algorithm: Monte Carlo policy gradient
- Actor-Critic: policy gradient + value function baseline
- Convergence analysis via two-timescale SA

**Thursday**: Prove policy gradient theorem: ∇_θ J(θ) = E^{π_θ}[∇_θ log π_θ(a|s) Q^{π_θ}(s,a)]

**Friday**:

- Review: Policy gradient directly optimizes performance; natural gradients use Fisher information
- Code: Implement REINFORCE on cart-pole with neural network policy

**Anchor Exercises**:

1. Derive policy gradient via likelihood ratio trick: ∇_θ E[f(X)] = E[f(X) ∇_θ log p_θ(X)]
2. Show that subtracting baseline b(s) from Q^π(s,a) doesn't change expectation but reduces variance

---

## Phase VIII: Advanced Topics and Continuous-Time Control (Weeks 40-43)

**Objective**: Connect everything to continuous-time formulations, viscosity solutions revisited, and modern deep RL perspective.

### Week 40: Continuous-Time MDPs and HJB Revisited

**Primary Reference**: Yong-Zhou Ch 4-5; Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:

- Controlled Markov processes in continuous time
- Infinitesimal generator and Dynkin formula
- HJB equation from dynamic programming principle

**Thursday**: Prove that value function V(t,x) satisfies HJB: ∂ₜV + sup_a {𝒜ᵃV + r(x,a)} = 0

**Friday**:

- Review: Continuous-time limit of discrete-time MDPs
- Code: Solve simple continuous-time control problem numerically (finite differences on HJB)

**Anchor Exercises**:

1. For Itô process dx = f(x,a)dt + σ(x,a)dW, compute generator 𝒜ᵃφ = f·∇φ + (1/2)Tr(σσ'∇²φ)
2. Derive HJB for LQR: dx = (Ax+Ba)dt, cost ∫(x'Qx + a'Ra)dt

### Week 41: Mean-Field Games and Multi-Agent RL

**Primary Reference**: Carmona-Delarue "Probabilistic Theory of Mean Field Games" Vol I (selected sections)

**Monday-Wednesday**:

- Mean-field limit of N-player games
- Nash equilibrium in mean-field games
- Coupled forward-backward PDEs (Fokker-Planck + HJB)

**Thursday**: Derive mean-field game system: HJB for value function + Fokker-Planck for population distribution

**Friday**:

- Review: Multi-agent RL at scale requires mean-field approximation
- Code: Simulate simple mean-field game (e.g., crowd motion), visualize Nash equilibrium

**Anchor Exercises**:

1. For linear-quadratic mean-field game, show Nash equilibrium is characterized by Riccati equation
2. Verify consistency: optimal control for representative agent matches aggregate behavior

### Week 42: Deep RL Theory - Approximation and Generalization

**Primary Reference**: Recent papers (Arora et al., Du et al. on neural tangent kernels in RL)

**Monday-Wednesday**:

- Function approximation with neural networks
- PAC bounds and sample complexity
- Neural Tangent Kernel perspective on gradient descent

**Thursday**: Survey proof sketch of sample complexity bound for fitted Q-iteration with function approximation

**Friday**:

- Review: Deep RL combines SA, FA, and SGD; theory still developing
- Code: Implement DQN on simple MDP, observe role of replay buffer and target network

**Anchor Exercises**:

1. For over-parameterized network, show training dynamics approximate kernel gradient descent in NTK regime
2. Derive Bellman error: ‖T_V - TπV‖ ≥ ‖V_ - V^π‖/(1+γ) (performance difference bound)

### Week 43: Synthesis - From Theory to Practice

**Primary Reference**: Your own notes and reflections; Survey paper of choice

**Monday-Wednesday**:

- Review key theorems: contraction mapping, ODE method, policy gradient theorem
- Survey modern algorithms: PPO, SAC, TD3 through lens of theory
- Gaps between theory and practice

**Thursday**: Write detailed summary connecting all phases: measure theory → FA → MDPs → bandits → SA → deep RL

**Friday**:

- Prepare for project phase
- Code: Set up codebase for AlphaZero-lite project

**Deliverable**: 10-page synthesis document connecting mathematical foundations to algorithmic practice

---

## Phase IX: Capstone Project (Weeks 44-48)

**Objective**: Implement sophisticated RL project demonstrating mastery of theory and practice.

### Project Proposal: AlphaZero-Lite for Reversi (Othello)

**Week 44-45: Environment and MCTS Implementation**

**Monday-Wednesday**:

- Implement Reversi game engine with board representation
- Monte Carlo Tree Search from first principles
- UCT formula and exploration-exploitation

**Thursday**: Prove UCT satisfies "regret minimization" property (relates to bandit theory from Phase VI)

**Friday**: Integrate and test MCTS on random play

**Deliverable**: Fully functional MCTS player that defeats random play

### Week 46-47: Neural Network Policy-Value Architecture**

**Monday-Wednesday**:

- Design convolutional architecture for board evaluation
- Policy head (action probabilities) and value head (position evaluation)
- Self-play data generation pipeline

**Thursday**: Implement training loop with mini-batch SGD

**Friday**: Test trained network as MCTS evaluation function

**Deliverable**: Trained network that improves MCTS play strength

### Week 48: Ablation Studies and Final Analysis**

**Monday-Wednesday**:

- Ablation: MCTS depth, network architecture, training iterations
- Compare AlphaZero-lite vs pure MCTS vs pure network
- ELO rating estimation

**Thursday**: Write detailed technical report connecting to theory:

- MCTS as UCB on tree (bandit theory)
- Value function approximation (Sobolev regularity of learned V)
- Policy improvement theorem (MDP theory)

**Friday**: Final presentation preparation

**Final Deliverable**:

1. Complete codebase on GitHub
2. 15-page technical report
3. Trained model checkpoint and evaluation results

---

## Daily Reflection Protocol

Each Friday, maintain a **learning log** with three components:

1. **Mathematical Insight** (3-5 sentences): What theorem or proof technique was most illuminating this week? Why?
    
2. **RL Connection** (2-3 sentences): How does this week's mathematics manifest in reinforcement learning algorithms?
    
3. **Open Questions** (1-2 questions): What remains unclear or invites deeper investigation?
    

This log becomes appendix material for your final synthesis document.

---

## Reference Library Organization

**Tier 1 (weekly use)**:

- Folland "Real Analysis"
- Brezis "Functional Analysis, Sobolev Spaces and PDEs"
- Puterman "Markov Decision Processes"
- Lattimore-Szepesvári "Bandit Algorithms"

**Tier 2 (regular reference)**:

- Durrett "Probability: Theory and Examples"
- Levin-Peres-Wilmer "Markov Chains and Mixing Times"
- Borkar "Stochastic Approximation"
- Bertsekas "Reinforcement Learning and Optimal Control"

**Tier 3 (specialized topics)**:

- Meyn-Tweedie "Markov Chains and Stochastic Stability"
- Yong-Zhou "Stochastic Controls"
- Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions"
- Sutton-Barto "Reinforcement Learning" (narrative guide)

---
