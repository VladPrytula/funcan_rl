48-Week Rigorous Syllabus: From Measure Theory to Reinforcement Learning

**Version**: 1.0 (Adjusted)  
**Duration**: 48 weeks (90 minutes/day, Monday-Friday)  
**Goal**: Rigorous mathematical foundations from measure theory through deep RL with capstone project

---

## Structural Principles

### Daily Time Allocation
**Total**: 90 minutes, Monday-Friday (weekends off)

### Adaptive Weekly Template

- **Monday-Wednesday**: 40 min reading + 40 min proof/exercise + 10 min reflection note
- **Thursday**: 30 min reading + 60 min extended proof or multi-part exercise  
- **Friday**: 20 min reading + 30 min proof review + 40 min coding synthesis

### Phase Structure
Six major phases with clear mathematical and RL objectives, plus capstone project phase.

---

## Phase I: Measure-Theoretic Foundations (Weeks 1-6)

**Objective**: Rapid but rigorous traversal of measure theory, emphasizing aspects critical for probability and RL. Given background in mathematical physics, this serves as structured review with targeted depth.

### Week 1: σ-Algebras, Measures, and Measurable Functions

**Primary Reference**: Folland §1.1-1.4

**Monday-Wednesday**:
- σ-algebras and their generation (monotone classes, Dynkin's π-λ theorem)
- Measures, outer measures, and Carathéodory extension theorem
- Measurable functions and their arithmetic/composition properties

**Thursday**: Extended proof of Carathéodory extension theorem
- **ADJUSTED**: Construction of Lebesgue measure from outer measure on rectangles → $\mathbb{R}^n$ only (not general product classes)
- Prove rectangles form semiring and generate $\mathcal{B}(\mathbb{R}^n)$ (~10 min)
- σ-additivity on Carathéodory-measurable sets (~40 min)
- Generalization note in reflection: extension works for arbitrary algebras

**Friday**: 
- Review: Why σ-algebras encode observability in RL
- Code: Implement discrete σ-algebra generator, visualize set operations

**Anchor Exercises**:
1. Prove the π-λ theorem: if $\mathcal{P}$ is a π-system and $\mathcal{L}$ is a λ-system with $\mathcal{P} \subseteq \mathcal{L}$, then $\sigma(\mathcal{P}) \subseteq \mathcal{L}$
2. Show that the Borel σ-algebra on $\mathbb{R}^n$ is generated by open balls (or by half-open rectangles)

### Week 2: Integration and Convergence Theorems

**Primary Reference**: Folland §2.1-2.4

**Monday-Wednesday**:
- Construction of the Lebesgue integral (simple functions → non-negative → general)
- Monotone Convergence Theorem (MCT)
- Fatou's Lemma and Dominated Convergence Theorem (DCT)

**Thursday**: Complete proof of DCT from first principles, emphasizing the role of dominating function

**Friday**:
- Review: How DCT enables interchange of expectation and limits in RL (e.g., policy evaluation)
- Code: Numerical experiments showing failure of pointwise limit interchange without domination

**Anchor Exercises**:
1. Prove that if $f_n \to f$ in $L^1$, there exists a subsequence converging almost everywhere
2. Construct explicit counterexamples showing necessity of each hypothesis in MCT and DCT

### Week 3: Product Measures and Fubini's Theorem

**Primary Reference**: Folland §2.4-2.5, Durrett §1.7

**Monday-Wednesday**:
- Product σ-algebras and product measures
- Fubini-Tonelli theorem (complete statement and proof strategy)
- Applications to transition kernels in MDPs

**Thursday**: **ADJUSTED** - Prove Tonelli completely, outline Fubini
- Full proof of Tonelli theorem for non-negative functions: $\int (\int f \, d\mu) \, d\nu = \int (\int f \, d\nu) \, d\mu$
- Include measurability of $x \mapsto \int f(x,y) \, d\nu(y)$ (state clearly if asserting via monotone class)
- Outline: How integrability assumption enables full Fubini for general functions (~15 min)

**Friday**: **ADJUSTED** - Counterexample exploration
- Code: Demonstrate Fubini failure without σ-finiteness (counting measure on uncountable sets)
- Code: Demonstrate failure without integrability (oscillating functions)
- Verify Fubini numerically for well-behaved examples

**Anchor Exercises**:
1. Prove the Tonelli form: if $f \geq 0$ is measurable on $X \times Y$, then iterated integrals are well-defined and equal
2. Construct counterexample showing σ-finiteness is necessary: use counting measure on uncountable set

### Week 4: Radon-Nikodym and Signed Measures  

**Primary Reference**: Folland §3.1-3.4

**Monday-Wednesday**:
- Signed measures and Hahn decomposition
- Absolute continuity and singularity of measures
- Radon-Nikodym theorem: existence of density $d\mu/d\nu$

**Thursday**: Complete proof of Radon-Nikodym theorem via maximal element in Hilbert space

**Friday**:
- Review: Radon-Nikodym underlies importance sampling and likelihood ratios in RL
- Code: Implement Radon-Nikodym density estimation numerically for simple examples

**Anchor Exercises**:
1. Prove the Hahn decomposition theorem for signed measures
2. Show that Radon-Nikodym derivative satisfies chain rule: if $\mu \ll \nu \ll \lambda$, then $d(\mu/\lambda) = (d\mu/d\nu)(d\nu/d\lambda)$ a.e.

### Week 5: $L^p$ Spaces and Duality

**Primary Reference**: Folland §6.1-6.3

**Monday-Wednesday**:
- Definition of $L^p(\mu)$ spaces, Hölder and Minkowski inequalities
- Completeness of $L^p$ (Riesz-Fischer theorem)
- Duality: $(L^p)^* \cong L^q$ for $p \in [1,\infty)$, $1/p + 1/q = 1$

**Thursday**: Prove Riesz representation theorem for $(L^p)^*$ when $1 < p < \infty$

**Friday**: **ADJUSTED** - Core proofs only
- Review: Value functions live in $L^\infty$, policy gradients involve $L^2$ inner products
- Code: Visualize convergence in different $L^p$ norms, demonstrate non-convergence in one norm vs another
- **L∞ non-separability to reflection note**: Briefly note this technical curiosity without full proof

**Anchor Exercises**:
1. Prove Hölder's inequality via convexity (Young's inequality for products)
2. Show that $L^p$ for $p < \infty$ is separable (on σ-finite measure spaces)

### Week 6: Probability Spaces and Conditional Expectation

**Primary Reference**: Durrett §1.1-1.6, §5.1

**Monday-Wednesday**:
- Probability spaces, random variables, and distributions
- Independence and product measures
- Conditional expectation as orthogonal projection in $L^2$
- Properties: tower property, taking out what is known

**Thursday**: Prove existence and uniqueness of conditional expectation using Hilbert space projection

**Friday**:
- Review: $\mathbb{E}[R|s,a]$ in MDPs is conditional expectation; tower property gives Bellman consistency
- Code: Simulate conditional expectation numerically, verify tower property empirically

**Anchor Exercises**:
1. Prove that $\mathbb{E}[X|\mathcal{G}]$ minimizes $\mathbb{E}[(X - Y)^2]$ over all $\mathcal{G}$-measurable $Y$
2. Establish the tower property: $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}] = \mathbb{E}[X|\mathcal{H}]$ when $\mathcal{H} \subseteq \mathcal{G}$

---

## Phase II: Markov Chains and Ergodic Theory (Weeks 7-12)

**Objective**: Master discrete-time Markov chains on finite and countable spaces, establishing the probabilistic backbone of MDPs.

### Week 7: Finite Markov Chains - Fundamentals

**Primary Reference**: Levin-Peres-Wilmer (LPW) Ch 1-4

**Monday-Wednesday**:
- Transition matrices and Chapman-Kolmogorov equation  
- Irreducibility, aperiodicity, and communication classes
- Stationary distributions and detailed balance

**Thursday**: Prove existence and uniqueness of stationary distribution for irreducible finite chains

**Friday**:
- Review: Stationary distribution is "average" state visitation under policy
- Code: Implement random walk on graphs, compute stationary distribution via eigenvalue and via simulation

**Anchor Exercises**:
1. Prove that irreducible finite chain has unique stationary distribution $\pi$ with $\pi_i > 0$ for all $i$
2. Show detailed balance ($\pi_i P_{ij} = \pi_j P_{ji}$) implies $\pi$ is stationary

### Week 8: Convergence and Mixing Times

**Primary Reference**: LPW Ch 5-7, 12

**Monday-Wednesday**:
- Total variation distance and coupling
- Convergence to stationarity: coupling lemma
- Mixing time definition and examples

**Thursday**: Prove coupling inequality: $d(t) \leq \mathbb{P}(X_1(t) \neq X_2(t))$ for chains started from coupling $(X_1,X_2)$

**Friday**:
- Review: Mixing time bounds sample complexity in RL exploration
- Code: Implement coupling for simple chains, visualize convergence to stationarity in TV norm

**Anchor Exercises**:
1. Compute mixing time for random walk on cycle graph $C_n$
2. Prove that for reversible chains, $\tau_{mix} \leq (\lambda_2^*)^{-1} \log(1/(\pi_{min}\delta))$ where $\lambda_2^* = 1 - \lambda_2$ is spectral gap

### Week 9: Markov Chain Monte Carlo

**Primary Reference**: LPW Ch 8-9, 13-14

**Monday**: **ADJUSTED** - Add 5-minute bridge
- MCMC overview and motivation
- **Bridge note**: "Why MCMC matters for exploration in RL" - connection to ε-greedy, Boltzmann, Thompson sampling

**Tuesday-Wednesday**:
- Metropolis-Hastings algorithm and its variants
- Gibbs sampling
- Path coupling method for bounding mixing

**Thursday**: **ADJUSTED** - Core proof only
- Prove correctness of Metropolis-Hastings: constructed chain has desired stationary distribution via detailed balance
- Keep detailed balance proof as core Thursday content

**Friday**: **ADJUSTED** - MCMC to optional synthesis
- **Optional but encouraged**: Implement Metropolis-Hastings for 2D Ising model, verify convergence
- **Alternative**: Implement simpler example (e.g., sampling from discrete distribution via M-H)
- **Key**: Preserve chain-mixing intuition; avoid rabbit holes in tuning/diagnostics
- **Future callback note**: "We'll revisit MCMC sampling in Week 32 (Thompson sampling = MCMC on posterior)"

**Anchor Exercises**:
1. Design Metropolis-Hastings algorithm to sample from $\pi(x) \propto \exp(-U(x))$ on finite state space
2. Prove detailed balance for Gibbs sampler on product space

### Week 10: Countable State Spaces - Transience and Recurrence  

**Primary Reference**: Durrett §6.1-6.4

**Monday-Wednesday**:
- Classification: transient vs recurrent states
- Null recurrence vs positive recurrence
- First passage times and hitting probabilities

**Thursday**: Prove Pólya's theorem: random walk on $\mathbb{Z}^d$ is recurrent iff $d \leq 2$

**Friday**:
- Review: Recurrence structure of state space affects exploration in RL
- Code: Simulate random walks in dimensions 1,2,3; empirically verify recurrence/transience

**Anchor Exercises**:
1. Show that $\mathbb{P}(\text{return to } 0|\text{start at } 0) = \sum_n P_{00}^{(n)} / (1 + \sum_n P_{00}^{(n)})$
2. Prove that in finite state space, not all states can be transient

### Week 11: Ergodic Theorems

**Primary Reference**: Durrett §6.5-6.7, Meyn-Tweedie Ch 17

**Monday**: **ADJUSTED** - Set expectations clearly
- Ergodic theorem overview
- **Clear statement**: We'll prove finite-state SLLN; Birkhoff's theorem will be stated with intuition
- Note on generalization: Birkhoff handles general measure-preserving transformations

**Tuesday-Wednesday**:
- Strong law for finite-state Markov chains
- Birkhoff's ergodic theorem (statement for measure-preserving transformations)
- Application to time averages in Markov chains

**Thursday**: **ADJUSTED** - Finite-state SLLN proof
- Prove finite-state Strong Law of Large Numbers via coupling convergence + DCT
- **Statement of Birkhoff**: For measure-preserving $T$ and $f \in L^1$, $(1/n)\sum_{k=0}^{n-1} f(T^k x) \to \mathbb{E}_\mu[f | \mathcal{I}]$ a.s.
- **Gap note** (~5 min): "For finite chains with unique stationary distribution, invariant σ-algebra $\mathcal{I}$ is trivial. For multichain MDPs (Week 28), we need full Birkhoff."

**Friday**: **ADJUSTED** - Numerical verification focus
- Verify ergodic theorem numerically for various chains
- Demonstrate failure for non-ergodic chain
- **Reflection**: "What we proved (finite-state SLLN) is corollary of Birkhoff. Distinction matters for: (1) multiple recurrent classes, (2) continuous-time MDPs. We'll see this in Weeks 28, 40."

**Anchor Exercises**:
1. For irreducible positive recurrent chain, prove $(1/n)\sum_{i=1}^n f(X_i) \to \sum_x \pi_x f(x)$ almost surely
2. Show necessity of positive recurrence: construct null recurrent chain where time average exists but differs from spatial average

### Week 12: General State Spaces - Introduction

**Primary Reference**: Meyn-Tweedie Ch 3-4 (selected topics)

**Monday-Wednesday**:
- Transition kernels on general measurable spaces
- Feller property and weak convergence
- Irreducibility and aperiodicity in general spaces

**Thursday**: Prove that Feller property implies weak continuity in initial distribution

**Friday**: **ADJUSTED** - Add concrete kernel example
- **Concrete example** (~12-15 min): Gaussian kernel on $[0,1]$ with reflection at boundaries
  - $K(x, dy) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-x)^2}{2\sigma^2}\right) dy$ with reflection: if $y < 0$, map to $-y$; if $y > 1$, map to $2-y$
  - Verify: (1) Feller property: $C([0,1]) \to C([0,1])$, (2) φ-irreducibility for φ = Lebesgue
- Code: Implement Markov chain with this kernel, visualize evolution of density

**Anchor Exercises**:
1. Define φ-irreducibility and show every finite chain is φ-irreducible for counting measure φ
2. Verify Feller property for Gaussian random walk: $X_{n+1} = X_n + \varepsilon_n$ with $\varepsilon_n \sim N(0,\sigma^2)$

---

## Phase III: Functional Analysis and Operator Theory (Weeks 13-18)

**Objective**: Establish the function space framework where Bellman operators and value functions reside. Deep dive into Banach and Hilbert spaces.

### Week 13: Normed Spaces and Banach Spaces

**Primary Reference**: Brezis Ch 1, §1.1-1.3

**Monday-Wednesday**:
- Definition and examples of normed vector spaces
- Completeness and Banach spaces
- Finite-dimensional spaces: equivalence of norms

**Thursday**: Prove all norms on $\mathbb{R}^n$ are equivalent (any two induce same topology)

**Friday**:
- Review: Space of bounded functions $\mathcal{B}(\mathcal{S})$ with supremum norm is Banach; value iteration works here
- Code: Visualize unit balls in $\mathbb{R}^2$ for various norms ($L^1$, $L^2$, $L^\infty$)

**Anchor Exercises**:
1. Show $C([0,1])$ with supremum norm is complete but $C([0,1])$ with $L^1$ norm is not
2. Prove that every finite-dimensional normed space is complete

### Week 14: Linear Operators and Dual Spaces

**Primary Reference**: Brezis Ch 1, §1.4-1.6

**Monday-Wednesday**:
- Bounded linear operators, operator norm
- Dual space and Hahn-Banach theorem (extension form)
- Weak and weak* topologies

**Thursday**: Prove Hahn-Banach theorem (geometric form: separating hyperplane)

**Friday**:
- Review: Linear value functions correspond to functionals; duality essential for policy gradients
- Code: Implement linear functionals on finite-dimensional spaces, visualize separating hyperplanes

**Anchor Exercises**:
1. If $X$ is Banach and $T:X \to Y$ linear with closed graph, prove $T$ is bounded (closed graph theorem)
2. Compute dual of $c_0$ (sequences converging to 0) and show $(c_0)^* \cong \ell^1$

### Week 15: Three Fundamental Theorems

**Primary Reference**: Brezis Ch 2, §2.1-2.7

**Monday-Wednesday**:
- Uniform Boundedness Principle (Banach-Steinhaus)
- Open Mapping Theorem and Inverse Mapping Theorem
- Closed Graph Theorem

**Thursday**: Prove Uniform Boundedness Principle via Baire category theorem

**Friday**:
- Review: UBP implies pointwise bounded sequence of Bellman backups is uniformly bounded
- Code: Construct example of pointwise bounded but not uniformly bounded sequence on infinite-dimensional space

**Anchor Exercises**:
1. Use UBP to show: if $\{T_n\} \subset \mathcal{L}(X,Y)$ and $T_n x$ converges for each $x$, then $T := \lim T_n$ is bounded
2. Prove that surjective bounded operator between Banach spaces is open

### Week 16: Hilbert Spaces

**Primary Reference**: Brezis Ch 5, §5.1-5.5

**Monday-Wednesday**:
- Inner product spaces and Cauchy-Schwarz inequality
- Orthogonality and projection theorem
- Riesz representation: $H^* \cong H$

**Thursday**: Prove projection theorem: every closed convex subset of Hilbert space contains unique nearest point to any given point

**Friday**:
- Review: Least-squares TD uses orthogonal projection in feature space (Hilbert space)
- Code: Implement orthogonal projection onto subspaces in $\mathbb{R}^n$, visualize geometry

**Anchor Exercises**:
1. Prove Parallelogram law: $\|x+y\|^2 + \|x-y\|^2 = 2(\|x\|^2 + \|y\|^2)$ characterizes inner product spaces
2. Show that weak convergence $x_n \rightharpoonup x$ and $\|x_n\| \to \|x\|$ together imply strong convergence $x_n \to x$

### Week 17: Compact Operators and Spectral Theory

**Primary Reference**: Brezis Ch 6, §6.1-6.5

**Monday-Wednesday**:
- Compact operators and Fredholm alternative
- Spectrum of bounded operator
- Spectral theorem for compact self-adjoint operators

**Thursday**: **ADJUSTED** - Spectral theorem full proof, Fredholm as application
- Full proof of spectral theorem: compact self-adjoint $T$ on Hilbert space $H$ has orthonormal eigenbasis (~50 min)
- Fredholm alternative as consequence (~10 min): Either $(I - K)u = f$ has unique solution for all $f$, or $(I - K)u = 0$ has nontrivial solution

**Friday**: **ADJUSTED** - Apply Fredholm to Bellman
- Review: Bellman equation $(I - \gamma P^\pi)V = R$ is Fredholm problem; when $\gamma = 1$, eigenvalue $\lambda = 1$ appears
- Code: Solve Bellman equation using eigendecomposition (20 min)
- Verify spectral theorem numerically: compute eigenvalues of discrete Laplacian (20 min)

**Anchor Exercises**:
1. Show composition of bounded operator with compact operator is compact
2. For compact self-adjoint $T$, prove $Tx = \lambda x$ with $\lambda \neq 0$ iff $\lambda$ is eigenvalue

### Week 18: Contraction Mappings and Fixed Points  

**Primary Reference**: Brezis Ch 9, §9.1-9.3; Puterman Appendix A

**Monday-Wednesday**:
- Metric spaces and completeness
- Banach Fixed Point Theorem (Contraction Mapping Theorem)
- Applications to differential and integral equations

**Thursday**: Prove Banach Fixed Point Theorem with explicit rate of convergence

**Friday**:
- Review: Bellman operator $T^\pi$ is γ-contraction in $L^\infty$, yielding unique fixed point $V^\pi$
- Code: Visualize fixed point iteration for various contractions, demonstrate linear convergence

**Anchor Exercises**:
1. Prove that if $T:X \to X$ is contraction with fixed point $x^*$, then $d(T^n x_0, x^*) \leq L^n d(x_0,x^*)/(1-L)$
2. Show non-expansive map on compact metric space has fixed point (Brouwer via approximation)

---

## Phase IV: Sobolev Spaces, PDEs, and Control Theory (Weeks 19-24)

**Objective**: Deep exploration of Sobolev spaces and variational methods, connecting to Hamilton-Jacobi-Bellman equations and continuous control. Leveraging PDE background for maximum depth.

### Week 19: Weak Derivatives and Sobolev Spaces $W^{k,p}$

**Primary Reference**: Brezis Ch 8, §8.1-8.3

**Monday-Wednesday**:
- Weak derivatives: definition via integration by parts
- Sobolev spaces $W^{k,p}(\Omega)$ and their basic properties
- Relationship between classical and weak solutions

**Thursday**: Prove that $W^{1,p}(\Omega)$ with $\|u\| = (\|u\|_p^p + \|\nabla u\|_p^p)^{1/p}$ is Banach space

**Friday**:
- Review: Neural networks produce $W^{1,\infty}$ functions (Lipschitz); Sobolev regularity affects approximation quality
- Code: Compute weak derivatives numerically via finite differences, compare with classical derivatives

**Anchor Exercises**:
1. Show that if $u \in W^{1,p}(\Omega)$ and $\Omega$ is bounded Lipschitz, then $\gamma u$ (trace) is well-defined in $L^p(\partial\Omega)$
2. Prove density of $C_c^\infty(\Omega)$ in $W^{1,p}(\Omega)$ when $\Omega$ has smooth boundary

### Week 20: Sobolev Embeddings and Compactness

**Primary Reference**: Brezis Ch 9, §9.1-9.4

**Monday-Wednesday**:
- Sobolev embedding theorem: $W^{1,p}(\mathbb{R}^n) \hookrightarrow L^q(\mathbb{R}^n)$ for appropriate $q$
- Rellich-Kondrachov compact embedding theorem
- Poincaré and Poincaré-Wirtinger inequalities

**Thursday**: Prove Gagliardo-Nirenberg-Sobolev inequality: if $p < n$, then $W^{1,p}(\mathbb{R}^n) \hookrightarrow L^{p^*}(\mathbb{R}^n)$ where $1/p^* = 1/p - 1/n$

**Friday**:
- Review: Sobolev embeddings ensure policies in $W^{1,p}$ have bounded supremum norm (needed for stability)
- Code: Numerically verify Sobolev embeddings for functions on unit interval

**Anchor Exercises**:
1. Prove Poincaré inequality on bounded connected $\Omega$: $\|u - \bar{u}\|_{L^2} \leq C\|\nabla u\|_{L^2}$ where $\bar{u}$ = mean of $u$
2. Show Rellich theorem implies: bounded sequence in $W^{1,p}(\Omega)$ has $L^p$-convergent subsequence (when $p < n^*$)

### Week 21: Variational Formulations and Weak Solutions

**Primary Reference**: Brezis Ch 8, §8.4-8.6; Ch 9, §9.5

**Monday-Wednesday**:
- Lax-Milgram theorem and weak solutions to elliptic PDEs
- Energy minimization and Euler-Lagrange equations
- Regularity theory: weak solutions are stronger

**Thursday**: Prove Lax-Milgram theorem: coercive continuous bilinear form on Hilbert space gives unique weak solution to $Lu = f$

**Friday**:
- Review: Value function in continuous control solves variational problem; weak formulation enables numerical methods
- Code: Solve Poisson equation $-\Delta u = f$ using finite elements (weak formulation)

**Anchor Exercises**:
1. For $-\Delta u + u = f$ in $\Omega$ with $u = 0$ on $\partial\Omega$, write weak formulation and prove existence/uniqueness via Lax-Milgram
2. Show that weak solution in $W_0^{1,2}(\Omega)$ of $-\Delta u = f \in L^2(\Omega)$ actually belongs to $W^{2,2}(\Omega)$ ($H^2$ regularity)

### Week 22: Hamilton-Jacobi Equations and Viscosity Solutions

**Primary Reference**: Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions" Ch 1-2

**Monday-Wednesday**:
- First-order Hamilton-Jacobi equations
- Characteristics and method of characteristics (when it works)
- Need for weak solutions: shocks and non-uniqueness

**Thursday**: Derive Hamilton-Jacobi-Bellman equation from dynamic programming principle in continuous time

**Friday**:
- Review: HJB equation is PDE satisfied by value function; viscosity solutions handle non-smoothness
- Code: Solve HJ equation $\partial_t u + H(\nabla u) = 0$ numerically via upwind scheme, observe shock formation

**Anchor Exercises**:
1. For $u_t + H(u_x) = 0$ with $H$ convex, derive Rankine-Hugoniot condition for shock propagation
2. Show that smooth solution to HJB equation gives optimal control via feedback law $a^*(s) = \arg\max H(s, \nabla V(s), a)$

### Week 23: Viscosity Solutions - Theory

**Primary Reference**: Bardi-Capuzzo Dolcetta Ch 3-4; or Crandall-Ishii-Lions (User's Guide)

**Monday-Wednesday**:
- Definition of viscosity sub/super solutions
- Comparison principle for viscosity solutions
- Existence and uniqueness theorems

**Thursday**: **ADJUSTED** - Comparison principle for coercive Lipschitz H
- Prove comparison principle for viscosity solutions of Hamilton-Jacobi equation
- **Restriction**: Coercive $H$ with Lipschitz Hamiltonian setting (simplified case, ~60 min)

**Friday**: **ADJUSTED** - 1D upwind + consider obstacle problem
- **Primary**: Implement Godunov scheme for 1D convex Hamilton-Jacobi, verify convergence to viscosity solution (20 min)
- **Consider adding** (~20 min): Obstacle problem in 2D gridworld
  - Value function with state constraints: $\max\{V(x,y), \psi(x,y)\}$ satisfies variational inequality
  - Gridworld implementation (finite differences)
  - Bridge to RL: safety constraints, inadmissible states
- Review: Viscosity solution is "right" notion for value function; DQN approximates viscosity solution

**Anchor Exercises**:
1. Verify that $u(x,t) = \min_y \{|x-y|^2/(2t) + u_0(y)\}$ is viscosity solution to heat equation for smooth $u_0$
2. Prove uniqueness of viscosity solution to $u + H(\nabla u) = 0$ when $H$ is coercive

### Week 24: Optimal Control and HJB in Continuous Time

**Primary Reference**: Yong-Zhou "Stochastic Controls" Ch 3-4, or Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:
- Controlled SDEs and diffusions
- Dynamic programming in continuous time
- HJB equation for stochastic control: $\rho V - \mathcal{A}V = \max_a \{r(s,a)\}$ where $\mathcal{A}$ is infinitesimal generator

**Thursday**: Derive HJB equation for stochastic control problem with Brownian noise via Itô's formula

**Friday**:
- Review: Actor-Critic algorithms solve HJB numerically; connection to policy gradient
- Code: Solve simple LQR problem analytically via HJB, compare with Riccati equation solution

**Anchor Exercises**:
1. For linear-quadratic regulator $dx = (Ax + Ba)dt + \sigma dW$ with cost $\int(x'Qx + a'Ra)dt$, derive Riccati equation from HJB
2. Verify verification theorem: if $V$ solves HJB with feedback control $a^*(s)$, then $V$ is value function

---

## Phase V: Markov Decision Processes and Dynamic Programming (Weeks 25-28)

**Objective**: Rigorous MDP theory where all prior mathematics pays dividends. Existence, uniqueness, convergence.

### Week 25: MDP Formalism and Bellman Equations

**Primary Reference**: Puterman Ch 3-4, Bertsekas Vol I Ch 1-2

**Monday-Wednesday**:
- Formal definition: $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ with measurability requirements
- Policies (deterministic, stationary, history-dependent)
- Value functions $V^\pi$ and $Q^\pi$; Bellman consistency equations
- Bellman optimality equation

**Thursday**: Prove that $V^\pi$ is unique fixed point of Bellman operator $T^\pi V = R^\pi + \gamma P^\pi V$

**Friday**:
- Review: Everything from measure theory to functional analysis culminates here
- Code: Implement gridworld MDP, verify Bellman equations numerically

**Anchor Exercises**:
1. Prove $T^\pi$ is γ-contraction in $(\mathcal{B}(\mathcal{S}), \|\cdot\|_\infty)$: $\|T^\pi V - T^\pi W\|_\infty \leq \gamma\|V - W\|_\infty$
2. Show that $Q^\pi$ satisfies $Q^\pi(s,a) = R(s,a) + \gamma\sum_{s'} P(s'|s,a) \sum_{a'} \pi(a'|s')Q^\pi(s',a')$

### Week 26: Optimal Policies and Value Iteration

**Primary Reference**: Puterman Ch 6, Bertsekas Vol I Ch 1

**Monday-Wednesday**:
- Existence of optimal policies for discounted infinite-horizon MDPs
- Value iteration algorithm and convergence analysis
- Optimality equations and policy improvement

**Thursday**: Prove convergence of value iteration: $V_n \to V^*$ at rate $\|V_n - V^*\| \leq \gamma^n\|V_0 - V^*\|/(1-\gamma)$

**Friday**:
- Review: Contraction mapping theorem is why value iteration works
- Code: Implement value iteration on various MDPs, plot convergence in sup norm

**Anchor Exercises**:
1. Prove policy improvement theorem: if $Q^\pi(s, \pi'(s)) \geq V^\pi(s)$ for all $s$, then $V^{\pi'} \geq V^\pi$
2. Show that greedy policy with respect to $V^*$ is optimal: $\pi^*(s) \in \arg\max_a Q^*(s,a)$

### Week 27: Policy Iteration and Linear Programming

**Primary Reference**: Puterman Ch 6-7

**Monday-Wednesday**:
- Policy iteration algorithm: evaluation + improvement
- Convergence in finite steps for finite MDPs
- Linear programming formulation of MDP

**Thursday**: Prove policy iteration converges in finite steps when $|\mathcal{S}| \times |\mathcal{A}| < \infty$

**Friday**:
- Review: Policy iteration is Newton's method on Bellman optimality
- Code: Implement policy iteration, compare with value iteration on computational efficiency

**Anchor Exercises**:
1. Prove that policy evaluation via solving $(I - \gamma P^\pi)V = R^\pi$ gives $V^\pi$
2. Formulate MDP as LP: maximize $\sum_s \nu(s)V(s)$ subject to $V(s) \geq R(s,a) + \gamma\sum_{s'} P(s'|s,a)V(s')$ for all $s,a$

### Week 28: Average Reward MDPs and Ergodic Theory

**Primary Reference**: Puterman Ch 8, Meyn-Tweedie Ch 17

**Monday**: **ADJUSTED** - Assert unichain explicitly
- Average reward criterion: $\lim (1/n)\mathbb{E}[\sum_{i=1}^n R(s_i,a_i)]$
- **Unichain assumption stated explicitly**: Single communicating class + one closed recurrent class
- **Brief note** (~5 min): Without unichain, gain is state-dependent (multichain case); transient states complicate bias

**Tuesday-Wednesday**:
- Existence of bias and gain under unichain assumption
- Connection to ergodic theorems for Markov chains
- Bellman equation for average reward: $g + h(s) = \max_a \{R(s,a) + \sum_{s'} P(s'|s,a)h(s')\}$

**Thursday**: Prove that for unichain MDP, average reward equals (stationary distribution)·(reward vector): $g = \sum_s \pi_s R_s$

**Friday**:
- Review: Average reward relevant for continuing tasks (no discounting)
- Code: Compute average reward via simulation, compare with solving Bellman equation for average reward

**Anchor Exercises**:
1. For unichain MDP with stationary policy $\pi$, prove gain $g^\pi = \sum_s \pi_s^\pi R_s^\pi$ where $\pi^\pi$ is stationary distribution
2. Derive Bellman equation for average reward: $g + h(s) = \max_a \{R(s,a) + \sum_{s'} P(s'|s,a)h(s')\}$ where $h$ is bias

---

## Phase VI: Bandit Algorithms (Weeks 29-33)

**Objective**: Systematic treatment of multi-armed bandits, contextual bandits, and connections to exploration-exploitation in RL.

### Week 29: Multi-Armed Bandits - Regret Framework

**Primary Reference**: Lattimore-Szepesvári (L-S) Ch 1-6

**Monday-Wednesday**:
- Stochastic bandits: arms, rewards, regret definition
- Lower bounds: instance-dependent and distribution-free (Lai-Robbins)
- Explore-then-commit and ε-greedy algorithms

**Thursday**: Prove Lai-Robbins lower bound: for any algorithm, max regret $\geq C \log(T)$ for some constant $C$

**Friday**:
- Review: Bandit problem is "one-state MDP"; exploration-exploitation tradeoff distilled
- Code: Implement ε-greedy on Bernoulli bandits, plot regret vs time

**Anchor Exercises**:
1. For $K$-armed bandit with Gaussian arms, compute optimal $\varepsilon(t)$ for ε-greedy to minimize regret
2. Prove regret bound for explore-then-commit: $R_T = O(T^{2/3})$

### Week 30: UCB Algorithms and Optimism

**Primary Reference**: L-S Ch 7-9

**Monday-Wednesday**:
- Upper Confidence Bound (UCB) algorithm
- Optimism in the face of uncertainty principle
- Chernoff-Hoeffding concentration inequalities

**Thursday**: Prove UCB regret bound: $R_T \leq O(\sqrt{KT \log T})$

**Friday**:
- Review: Optimism principle generalizes to RL (optimistic value initialization)
- Code: Implement UCB, compare with ε-greedy on various reward distributions

**Anchor Exercises**:
1. Derive Chernoff bound: $\mathbb{P}(\bar{X}_n \geq \mu + \varepsilon) \leq \exp(-2n\varepsilon^2)$ for iid bounded random variables
2. Prove that UCB with $\alpha_t = \sqrt{(2 \log t)/n_a(t)}$ achieves $O(\log T)$ regret for subgaussian arms

### Week 31: Thompson Sampling and Bayesian Bandits

**Primary Reference**: L-S Ch 10-11; Russo et al. "Tutorial on Thompson Sampling"

**Monday-Wednesday**:
- Bayesian approach to bandits: prior, posterior updates
- Thompson Sampling algorithm
- Information-theoretic analysis

**Thursday**: Prove Bayesian regret bound for Thompson Sampling in Bernoulli bandit case

**Friday**:
- Review: Thompson Sampling is exploration via posterior sampling; connects to probability matching
- **Callback to Week 9**: Thompson Sampling = MCMC on posterior distribution
- Code: Implement Thompson Sampling with Beta-Bernoulli conjugate prior, visualize posterior evolution

**Anchor Exercises**:
1. For Beta$(α,β)$ prior and Bernoulli observations, derive posterior Beta$(α+\text{successes}, β+\text{failures})$
2. Show Thompson Sampling is probability matching: $\mathbb{P}(\text{arm } k \text{ selected}) = \mathbb{P}(\text{arm } k \text{ is optimal} | \text{data})$

### Week 32: Contextual Bandits and Linear Models

**Primary Reference**: L-S Ch 19-20

**Monday-Wednesday**:
- Contextual bandits: state information before action selection
- Linear contextual bandits: rewards are linear in features
- LinUCB algorithm and regret analysis

**Thursday**: Prove regret bound for LinUCB: $R_T = O(d\sqrt{T \log T})$ where $d$ is feature dimension

**Friday**:
- Review: Contextual bandits bridge to RL (full MDP is contextual bandit with state transitions)
- Code: Implement LinUCB on synthetic contextual bandit problem

**Anchor Exercises**:
1. Derive ridge regression solution $\hat{\theta} = (X'X + \lambda I)^{-1}X'y$ for linear model
2. Prove elliptical potential lemma: $\sum_t \|x_t\|_{V_t^{-1}}^2 \leq 2d \log(1 + t/(\lambda d))$ where $V_t = \sum_i x_i x_i' + \lambda I$

### Week 33: Advanced Topics in Bandits

**Primary Reference**: L-S Ch 15-16 (adversarial bandits), Ch 27-28 (structured bandits)

**Monday-Wednesday**:
- Adversarial bandits and Exp3 algorithm
- Structured bandits (graph feedback, combinatorial)
- Connections to online learning and no-regret dynamics

**Thursday**: Prove Exp3 regret bound: $R_T = O(\sqrt{KT \log K})$ against adaptive adversary

**Friday**:
- Review: Adversarial bandits prepare for minimax RL (robust control)
- Code: Implement Exp3, compare with UCB under adversarial vs stochastic rewards

**Anchor Exercises**:
1. Derive multiplicative weights update: $w_{t+1}(k) = w_t(k) \exp(\eta \ell_t(k))$
2. Prove that Exp3 with $\eta = \sqrt{\log K/(KT)}$ achieves $O(\sqrt{KT \log K})$ regret

---

## Phase VII: Stochastic Approximation and RL Algorithms (Weeks 34-39)

**Objective**: Rigorous convergence theory for temporal difference learning via stochastic approximation and ODE methods.

### Stochastic Approximation Hygiene Checklist

**CRITICAL**: Before analyzing any SA algorithm, verify:
```
□ Step sizes: ∑αₙ = ∞, ∑αₙ² < ∞ (Robbins-Monro)
□ Noise structure: 𝔼[Mₙ₊₁|ℱₙ] = 0 (martingale difference)
□ Noise boundedness: 𝔼[|Mₙ₊₁|²|ℱₙ] ≤ C(1 + ‖θₙ‖²) (controlled variance)
□ ODE structure: θ̇ = h(θ) where h is Lipschitz continuous
□ Stability: ODE has global attractor (Liapunov function exists)
□ Projection (if needed): Iterates stay in compact set Θ via projection
□ Two-timescale (if applicable): βₙ/αₙ → 0 for faster timescale
```

**Usage**: Refer to this checklist every time you analyze an SA algorithm (TD, Q-learning, actor-critic).

### Week 34: Robbins-Monro and Stochastic Approximation

**Primary Reference**: Borkar "Stochastic Approximation" Ch 1-2, or Kushner-Yin Ch 1-5

**Monday**: **ADJUSTED** - Introduce SA hygiene checklist
- Read checklist, understand each condition
- Overview of Robbins-Monro algorithm: $\theta_{n+1} = \theta_n + \alpha_n(h(\theta_n) + M_{n+1})$

**Tuesday-Wednesday**:
- Step size conditions: $\sum\alpha_n = \infty$, $\sum\alpha_n^2 < \infty$
- Martingale convergence theorems (Doob, martingale difference sequences)
- Robbins-Monro convergence theorem

**Thursday**: Prove Robbins-Monro convergence: if $\mathbb{E}[M_{n+1}|\mathcal{F}_n] = 0$ and $h$ is Lipschitz with unique root $\theta^*$, then $\theta_n \to \theta^*$ almost surely

**Friday**:
- Review: TD learning is stochastic approximation to solve Bellman equation
- Code: Implement Robbins-Monro for finding root of simple function, vary step sizes
- **Reflection**: Which checklist conditions were verified in the proof? What happens if step sizes don't satisfy conditions?

**Anchor Exercises**:
1. Verify that $\alpha_n = 1/n$ satisfies Robbins-Monro step size conditions
2. Prove that if $\sum\alpha_n^2 < \infty$ and $\sum\alpha_n|M_n| < \infty$ a.s., then $\sum\alpha_n M_n$ converges a.s.

### Week 35: ODE Method for Stochastic Approximation

**Primary Reference**: Borkar Ch 3-5

**Monday-Wednesday**:
- Trajectory approximation by ODEs: $\dot{\theta} = h(\theta)$
- Liapunov function methods for stability
- Two-timescale stochastic approximation

**Thursday**: Prove ODE method: trajectory of $\theta_n$ tracks ODE solution, convergence follows from ODE stability

**Friday**:
- Review: ODE method is primary tool for analyzing TD convergence
- Code: Simulate SA and corresponding ODE side-by-side, observe tracking
- **Reflection**: Return to SA checklist—how does ODE method use each condition?

**Anchor Exercises**:
1. For $\theta_{n+1} = \theta_n - \alpha_n\nabla f(\theta_n) + \alpha_n M_{n+1}$, show that ODE is $\dot{\theta} = -\nabla f(\theta)$, which converges to local minimum of $f$
2. Analyze two-timescale SA: $\theta_{n+1} = \theta_n + \alpha_n h_\theta(\theta_n, \omega_n)$, $\omega_{n+1} = \omega_n + \beta_n h_\omega(\theta_n, \omega_n)$ with $\beta_n/\alpha_n \to 0$

### Week 36: Temporal Difference Learning - Tabular Case

**Primary Reference**: Tsitsiklis-Van Roy, Sutton-Barto Ch 6 with rigor from Szepesvári "Algorithms for RL"

**Monday**: **ADJUSTED** - Check SA hygiene for TD(0)
- TD(0) algorithm for policy evaluation
- **Apply SA checklist**: Identify $h(\theta)$, verify step sizes, check noise structure

**Tuesday-Wednesday**:
- Fixed point characterization: TD solves projected Bellman equation
- Convergence proof via stochastic approximation

**Thursday**: Prove TD(0) converges to $V^\pi$ when state space is finite

**Friday**:
- Review: TD is sample-based iterative method for solving linear system
- Code: Implement tabular TD(0) on random walk, compare with Monte Carlo
- **Reflection**: Which SA conditions were hardest to verify for TD? What breaks if we violate condition X?

**Anchor Exercises**:
1. Show TD(0) update is $V_{t+1}(s_t) = V_t(s_t) + \alpha_t(r_t + \gamma V_t(s_{t+1}) - V_t(s_t))$ and identify as Robbins-Monro
2. Prove that for finite MDP under policy $\pi$, TD converges to $V^\pi$ using ODE method with ODE $\dot{V} = \mathbb{E}[\delta^\pi(s)]$

### Week 37: TD with Function Approximation - Linear Case

**Primary Reference**: Tsitsiklis-Van Roy "Analysis of Temporal-Difference Learning"; Sutton-Barto Ch 9

**Monday**: **ADJUSTED** - SA hygiene for linear TD
- Linear function approximation: $\hat{V}(s; \theta) = \phi(s)'\theta$
- TD with linear FA: semi-gradient update
- **SA checklist application**: Projection ensures bounded iterates

**Tuesday-Wednesday**:
- Projected Bellman equation and fixed point
- Convergence to fixed point of $\Pi T^\pi$

**Thursday**: Prove convergence of linear TD to fixed point of projected Bellman operator $\Pi T^\pi$

**Friday**:
- Review: Projection onto feature space introduces bias; fixed point $\neq V^\pi$ in general
- Code: Implement linear TD on mountain car with tile coding features
- **Reflection**: How does projection affect SA convergence? Which checklist items change?

**Anchor Exercises**:
1. For linear TD, show fixed point satisfies $\Phi\theta^* = \Pi(R + \gamma P\Phi\theta^*)$ where $\Pi$ projects onto span$(\Phi)$
2. Verify that $\|\theta^* - \bar{\theta}\| \leq (1/(1-\gamma)) \|(I-\Pi)V^\pi\|$ where $\bar{\theta}$ minimizes $\|V^\pi - \Phi\theta\|$ (approximation error bound)

### Week 38: Q-Learning and Off-Policy Learning

**Primary Reference**: Watkins thesis; Szepesvári Ch 6; Sutton-Barto Ch 6

**Monday**: **ADJUSTED** - SA hygiene for Q-learning
- Q-learning algorithm: off-policy TD control
- **SA checklist**: Note that $\max_{a'} Q_t(s',a')$ breaks linearity—need careful analysis

**Tuesday-Wednesday**:
- Convergence proof for tabular Q-learning
- Importance sampling for off-policy evaluation

**Thursday**: Prove Q-learning converges to $Q^*$ when all state-action pairs visited infinitely often

**Friday**:
- Review: Q-learning is SA solving Bellman optimality equation; off-policy enables exploration
- Code: Implement tabular Q-learning on cliff walking, compare with SARSA
- **Reflection**: Why does max operation not violate Lipschitz condition in SA checklist?

**Anchor Exercises**:
1. Show Q-learning update $Q_{t+1}(s,a) = Q_t(s,a) + \alpha[r + \gamma \max_{a'} Q_t(s',a') - Q_t(s,a)]$ is SA with $h(Q) = T^*Q - Q$
2. Prove importance sampling estimator $\rho = \prod_t (\pi(a_i|s_i)/b(a_i|s_i))$ is unbiased for $\mathbb{E}^\pi[G]$ given trajectory from $b$

### Week 39: Policy Gradient Methods

**Primary Reference**: Sutton et al. "Policy Gradient Methods"; Agarwal et al. "Theory of Reinforcement Learning"

**Monday**: **ADJUSTED** - SA hygiene for policy gradient
- REINFORCE algorithm: Monte Carlo policy gradient
- **SA checklist**: Actor-Critic is two-timescale SA

**Tuesday-Wednesday**:
- Actor-Critic: policy gradient + value function baseline
- Convergence analysis via two-timescale SA

**Thursday**: Prove policy gradient theorem: $\nabla_\theta J(\theta) = \mathbb{E}^{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$

**Friday**:
- Review: Policy gradient directly optimizes performance; natural gradients use Fisher information
- Code: Implement REINFORCE on cart-pole with neural network policy
- **Reflection**: In Actor-Critic, which timescale is faster? Why does two-timescale SA apply?

**Anchor Exercises**:
1. Derive policy gradient via likelihood ratio trick: $\nabla_\theta \mathbb{E}[f(X)] = \mathbb{E}[f(X) \nabla_\theta \log p_\theta(X)]$
2. Show that subtracting baseline $b(s)$ from $Q^\pi(s,a)$ doesn't change expectation but reduces variance

---

## Phase VIII: Advanced Topics and Continuous-Time Control (Weeks 40-43)

**Objective**: Connect everything to continuous-time formulations, viscosity solutions revisited, and modern deep RL perspective.

### Week 40: Continuous-Time MDPs and HJB Revisited

**Primary Reference**: Yong-Zhou Ch 4-5; Pham "Continuous-time Stochastic Control"

**Monday-Wednesday**:
- Controlled Markov processes in continuous time
- Infinitesimal generator and Dynkin formula
- HJB equation from dynamic programming principle

**Thursday**: Prove that value function $V(t,x)$ satisfies HJB: $\partial_t V + \sup_a \{\mathcal{A}^a V + r(x,a)\} = 0$

**Friday**:
- Review: Continuous-time limit of discrete-time MDPs
- Code: Solve simple continuous-time control problem numerically (finite differences on HJB)

**Anchor Exercises**:
1. For Itô process $dx = f(x,a)dt + \sigma(x,a)dW$, compute generator $\mathcal{A}^a\phi = f \cdot \nabla\phi + (1/2)\text{Tr}(\sigma\sigma'\nabla^2\phi)$
2. Derive HJB for LQR: $dx = (Ax+Ba)dt$, cost $\int(x'Qx + a'Ra)dt$

### Week 41: Mean-Field Games and Multi-Agent RL

**Primary Reference**: Carmona-Delarue "Probabilistic Theory of Mean Field Games" Vol I (selected sections)

**Monday-Wednesday**:
- Mean-field limit of $N$-player games
- Nash equilibrium in mean-field games
- Coupled forward-backward PDEs (Fokker-Planck + HJB)

**Thursday**: Derive mean-field game system: HJB for value function + Fokker-Planck for population distribution

**Friday**:
- Review: Multi-agent RL at scale requires mean-field approximation
- Code: Simulate simple mean-field game (e.g., crowd motion), visualize Nash equilibrium

**Anchor Exercises**:
1. For linear-quadratic mean-field game, show Nash equilibrium is characterized by Riccati equation
2. Verify consistency: optimal control for representative agent matches aggregate behavior

### Week 42: Deep RL Theory - Approximation and Generalization

**Primary Reference**: Recent papers (Arora et al., Du et al. on neural tangent kernels in RL)

**Monday-Wednesday**:
- Function approximation with neural networks
- PAC bounds and sample complexity
- Neural Tangent Kernel perspective on gradient descent

**Thursday**: Survey proof sketch of sample complexity bound for fitted Q-iteration with function approximation

**Friday**:
- Review: Deep RL combines SA, FA, and SGD; theory still developing
- Code: Implement DQN on simple MDP, observe role of replay buffer and target network

**Anchor Exercises**:
1. For over-parameterized network, show training dynamics approximate kernel gradient descent in NTK regime
2. Derive Bellman error: $\|T^*V - T^\pi V\| \geq \|V^* - V^\pi\|/(1+\gamma)$ (performance difference bound)

### Week 43: Synthesis - From Theory to Practice

**Primary Reference**: Your own notes and reflections; Survey paper of choice

**Monday-Wednesday**:
- Review key theorems: contraction mapping, ODE method, policy gradient theorem
- Survey modern algorithms: PPO, SAC, TD3 through lens of theory
- Gaps between theory and practice

**Thursday**: Write detailed summary connecting all phases: measure theory → FA → MDPs → bandits → SA → deep RL

**Friday**:
- Prepare for project phase
- Code: Set up codebase for AlphaZero-lite project

**Deliverable**: 10-page synthesis document connecting mathematical foundations to algorithmic practice

---

## Phase IX: Capstone Project (Weeks 44-48)

**Objective**: Implement sophisticated RL project demonstrating mastery of theory and practice.

### Project: AlphaZero-Lite for Reversi (Othello)

### Week 44-45: Environment and MCTS Implementation

**Monday-Wednesday**:
- Implement Reversi game engine with board representation
- Monte Carlo Tree Search from first principles
- UCT formula and exploration-exploitation

**Thursday**: Prove UCT satisfies "regret minimization" property (relates to bandit theory from Phase VI)

**Friday**: Integrate and test MCTS on random play

**Deliverable**: Fully functional MCTS player that defeats random play

### Week 46-47: Neural Network Policy-Value Architecture

**Monday-Wednesday**:
- Design convolutional architecture for board evaluation
- Policy head (action probabilities) and value head (position evaluation)
- Self-play data generation pipeline

**Thursday**: Implement training loop with mini-batch SGD

**Friday**: Test trained network as MCTS evaluation function

**Deliverable**: Trained network that improves MCTS play strength

### Week 48: Ablation Studies and Final Analysis

**Monday-Wednesday**:
- Ablation: MCTS depth, network architecture, training iterations
- Compare AlphaZero-lite vs pure MCTS vs pure network
- ELO rating estimation

**Thursday**: Write detailed technical report connecting to theory:
- MCTS as UCB on tree (bandit theory)
- Value function approximation (Sobolev regularity of learned $V$)
- Policy improvement theorem (MDP theory)

**Friday**: Final presentation preparation

**Final Deliverable**: 
1. Complete codebase on GitHub
2. 15-page technical report
3. Trained model checkpoint and evaluation results

---

## Daily Reflection Protocol

Each Friday, maintain a **learning log** with three components:

1. **Mathematical Insight** (3-5 sentences): What theorem or proof technique was most illuminating this week? Why?

2. **RL Connection** (2-3 sentences): How does this week's mathematics manifest in reinforcement learning algorithms?

3. **Open Questions** (1-2 questions): What remains unclear or invites deeper investigation?

This log becomes appendix material for your final synthesis document.

---

## Postponed Generalizations Log

At the end of each week, maintain a log of theoretical generalizations that were simplified or postponed:

**Format**:
- **What was postponed**: Brief description
- **Why**: Time constraint vs. relevance to RL
- **Where to return**: Citation/reference if needed later
- **When it matters**: Specific scenarios where full generality is required

**Example entries**:
- Week 1: General Carathéodory extension → postponed to focus on Lebesgue construction
- Week 11: Full Birkhoff ergodic theorem → stated without proof; matters for multichain MDPs (Week 28)

---

## Reference Library Organization

### Tier 1 (weekly use)
- Folland "Real Analysis"
- Brezis "Functional Analysis, Sobolev Spaces and PDEs"  
- Puterman "Markov Decision Processes"
- Lattimore-Szepesvári "Bandit Algorithms"

### Tier 2 (regular reference)
- Durrett "Probability: Theory and Examples"
- Levin-Peres-Wilmer "Markov Chains and Mixing Times"
- Borkar "Stochastic Approximation"
- Bertsekas "Reinforcement Learning and Optimal Control"

### Tier 3 (specialized topics)
- Meyn-Tweedie "Markov Chains and Stochastic Stability"
- Yong-Zhou "Stochastic Controls"
- Bardi-Capuzzo Dolcetta "Optimal Control and Viscosity Solutions"
- Sutton-Barto "Reinforcement Learning" (narrative guide)

---

## Weekly Checklist Templates

### For Measure Theory & Probability (Weeks 1-6, 10-12)
```
□ Definitions stated precisely with all quantifiers
□ Counterexamples constructed for "necessity" results
□ Coding verifies theorem numerically
□ RL connection articulated explicitly
```

### For Functional Analysis (Weeks 13-18)
```
□ Normed space structure identified in RL context
□ Convergence mode specified (strong/weak/pointwise)
□ Operator properties verified (bounded/compact/contraction)
□ Bellman operator example provided
```

### For Markov Chains (Weeks 7-9)
```
□ Irreducibility and aperiodicity checked
□ Stationary distribution computed (analytical + numerical)
□ Mixing time estimated
□ Connection to RL exploration noted
```

### For Stochastic Approximation (Weeks 34-39)
**Use SA Hygiene Checklist** (provided in Phase VII introduction)

---

## Meta-Cognitive Strategies

### When Stuck on a Proof (>20 minutes)
1. State what you're trying to prove in your own words
2. Write down all hypotheses explicitly
3. Try the contrapositive or a special case
4. Look for a "good sets" or "truncation" argument
5. Consult the reflection notes from related weeks
6. Move to coding/numerical verification, return later

### When Implementation Fails
1. Check against SA hygiene checklist (if applicable)
2. Verify all measure-theoretic assumptions (measurability, integrability)
3. Test on trivial case (2-state MDP, 1D grid)
4. Compare with closed-form solution if available
5. Visualize intermediate quantities

### Friday Review Questions
- What was the "big theorem" this week?
- What technique will I use again?
- What prerequisite from earlier weeks was essential?
- Where does this appear in modern RL papers?

---

**End of Syllabus**

*Version 1.0 - Adjusted with surgical refinements*  
*Total duration: 48 weeks × 5 days × 90 minutes = 360 hours*  
*Prepared for: Rigorous journey from measure theory to deep reinforcement learning*
